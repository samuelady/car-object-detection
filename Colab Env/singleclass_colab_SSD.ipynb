{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPHagtbPvUkromVawQvFTwt"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Object Detection with SSD (Single Shot MultiBox Detector)\n","\n","# This notebook demonstrates training an SSD model for object detection, specifically for car detection.\n","# It covers data loading, model training, evaluation, and inference."],"metadata":{"id":"N628pOuOgVtM"}},{"cell_type":"code","execution_count":28,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rB8Mp5ZIgN_5","executionInfo":{"status":"ok","timestamp":1748857661330,"user_tz":-420,"elapsed":3642,"user":{"displayName":"Samuel Ady Sanjaya","userId":"15801194796254517624"}},"outputId":"c7ada944-0e80-488d-8990-5aa6c5ab6873"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["# === PATH DEFINITIONS ===\n","import os # Import os here as it's used immediately\n","\n","DRIVE_MOUNT_POINT = '/content/drive'\n","BASE_PROJECT_PATH = '/content/drive/MyDrive/Projects/Car Detection v2/' # MODIFY IF YOUR BASE PATH IS DIFFERENT\n","DATASET_BASE_PATH = os.path.join(BASE_PROJECT_PATH, 'Single Class Data') #Roboflow dataset\n","\n","# Training data paths\n","TRAIN_IMAGES_PATH = os.path.join(DATASET_BASE_PATH, 'train/images/')\n","TRAIN_LABELS_PATH = os.path.join(DATASET_BASE_PATH, 'train/labels/')\n","\n","# Validation data paths\n","VALID_IMAGES_PATH = os.path.join(DATASET_BASE_PATH, 'valid/images/')\n","VALID_LABELS_PATH = os.path.join(DATASET_BASE_PATH, 'valid/labels/')\n","\n","# Test data path\n","TEST_IMAGES_DIR = os.path.join(DATASET_BASE_PATH, 'test/images') # For inference/testing\n","\n","# Dataset configuration file\n","DATASET_YAML_PATH = os.path.join(DATASET_BASE_PATH, 'data.yaml')\n","\n","# Model training output paths\n","MODEL_OUTPUT_BASE_PATH = os.path.join(BASE_PROJECT_PATH, 'Colab Env/Model History')\n","EXPERIMENT_NAME = 'singleclass_colab_YOLOv8' # Choose a descriptive name\n","FULL_EXPERIMENT_PATH = os.path.join(MODEL_OUTPUT_BASE_PATH, EXPERIMENT_NAME)\n","\n","# Paths to specific files generated during training\n","BEST_WEIGHTS_PATH = os.path.join(FULL_EXPERIMENT_PATH, 'weights/best.pt')\n","RESULTS_CSV_PATH = os.path.join(FULL_EXPERIMENT_PATH, 'results.csv')\n","CONFUSION_MATRIX_IMG_PATH = os.path.join(FULL_EXPERIMENT_PATH, 'confusion_matrix.png')\n","# F1_CURVE_IMG_PATH = os.path.join(FULL_EXPERIMENT_PATH, 'F1_curve.png') # Example if you need other plots\n","# P_CURVE_IMG_PATH = os.path.join(FULL_EXPERIMENT_PATH, 'P_curve.png')\n","# PR_CURVE_IMG_PATH = os.path.join(FULL_EXPERIMENT_PATH, 'PR_curve.png')\n","# R_CURVE_IMG_PATH = os.path.join(FULL_EXPERIMENT_PATH, 'R_curve.png')\n","\n","# Paths to video test\n","VIDEO_TEST = os.path.join(BASE_PROJECT_PATH, 'Video Test')\n","VIDEO_TEST_PREDICTION = os.path.join(BASE_PROJECT_PATH, 'Video Test Prediction Colab Env')\n","\n","print(f\"Base Project Path: {BASE_PROJECT_PATH}\")\n","print(f\"Dataset Path: {DATASET_BASE_PATH}\")\n","print(f\"Dataset YAML Path: {DATASET_YAML_PATH}\")\n","print(f\"Model Output Path: {FULL_EXPERIMENT_PATH}\")\n","print(f\"Best Weights will be saved to: {BEST_WEIGHTS_PATH}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bHd7K9BPiC6Z","executionInfo":{"status":"ok","timestamp":1748857661338,"user_tz":-420,"elapsed":7,"user":{"displayName":"Samuel Ady Sanjaya","userId":"15801194796254517624"}},"outputId":"bd076fdb-55db-4607-de80-ced59b89e1c4"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["Base Project Path: /content/drive/MyDrive/Projects/Car Detection v2/\n","Dataset Path: /content/drive/MyDrive/Projects/Car Detection v2/Single Class Data\n","Dataset YAML Path: /content/drive/MyDrive/Projects/Car Detection v2/Single Class Data/data.yaml\n","Model Output Path: /content/drive/MyDrive/Projects/Car Detection v2/Colab Env/Model History/singleclass_colab_YOLOv8\n","Best Weights will be saved to: /content/drive/MyDrive/Projects/Car Detection v2/Colab Env/Model History/singleclass_colab_YOLOv8/weights/best.pt\n"]}]},{"cell_type":"code","source":["!pip install torch torchvision opencv-python matplotlib seaborn pandas\n","\n","# === System & I/O ===\n","import random\n","import glob\n","import json # You might need this if data.yaml maps to a JSON config for SSD\n","\n","# === Data & Visualization ===\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import matplotlib.image as mpimg\n","\n","# === Computer Vision & Model ===\n","import cv2\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","import torchvision\n","from torchvision.transforms import functional as F\n","from torchvision.models.detection import (\n","    ssd300_vgg16,\n","    SSD300_VGG16_Weights,\n","    _utils as det_utils\n",")\n","# If using MobileNetV3 as backbone for SSD, you'd combine:\n","from torchvision.models.detection.anchor_utils import AnchorGenerator\n","from torchvision.models.detection.ssd import SSDHead\n","from torchvision.models.detection import _utils\n","from torchvision.models import mobilenet_v3_large, MobileNet_V3_Large_Weights\n","\n","\n","# Configure plotting style for seaborn and matplotlib\n","sns.set_style('darkgrid')\n","%matplotlib inline"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"owSY1rXViJwA","executionInfo":{"status":"ok","timestamp":1748857663616,"user_tz":-420,"elapsed":2277,"user":{"displayName":"Samuel Ady Sanjaya","userId":"15801194796254517624"}},"outputId":"068b0475-b728-45d0-85ab-3171e4d9a123"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.7.0)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.22.0)\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n","Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch) (1.14.0)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.11/dist-packages (from torch) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /usr/local/lib/python3.11/dist-packages (from torch) (9.5.1.17)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.11/dist-packages (from torch) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.11/dist-packages (from torch) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.11/dist-packages (from torch) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.11/dist-packages (from torch) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.3)\n","Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /usr/local/lib/python3.11/dist-packages (from torch) (2.26.2)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.11/dist-packages (from torch) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.11/dist-packages (from torch) (1.11.1.6)\n","Requirement already satisfied: triton==3.3.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.3.0)\n","Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.3.0->torch) (75.2.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.0)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n"]}]},{"cell_type":"code","source":["# Verify dataset directory\n","if os.path.exists(DATASET_BASE_PATH):\n","    print(f\"Dataset directory listing for: {DATASET_BASE_PATH}\")\n","    print(os.listdir(DATASET_BASE_PATH))\n","else:\n","    print(f\"ERROR: Dataset base path not found: {DATASET_BASE_PATH}\")\n","\n","if os.path.exists(TRAIN_IMAGES_PATH):\n","    print(f\"\\nTrain images directory listing for: {TRAIN_IMAGES_PATH}\")\n","    print(os.listdir(TRAIN_IMAGES_PATH)[:5]) # Print first 5 for brevity\n","else:\n","    print(f\"ERROR: Train images path not found: {TRAIN_IMAGES_PATH}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"on25Um4DiVWK","executionInfo":{"status":"ok","timestamp":1748857663652,"user_tz":-420,"elapsed":33,"user":{"displayName":"Samuel Ady Sanjaya","userId":"15801194796254517624"}},"outputId":"f65970d9-7983-4b60-b718-b5fc105928f6"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["Dataset directory listing for: /content/drive/MyDrive/Projects/Car Detection v2/Single Class Data\n","['README.dataset.txt', 'data.yaml', 'README.roboflow.txt', '.DS_Store', 'valid', 'test', 'train']\n","\n","Train images directory listing for: /content/drive/MyDrive/Projects/Car Detection v2/Single Class Data/train/images/\n","['screenshot_1697611055-8898516_jpg.rf.4ab0816a8d5cdbec1b93f10d04c39ed8.jpg', 'screenshot_1697681386-8348253_jpg.rf.fa840d186280e480551afe5acd0d64d3.jpg', 'screenshot_1697682787-6729238_jpg.rf.8799931f02714a887bb96e0a2cf2292d.jpg', 'frame_330_jpg.rf.80d8c478551e0efd782d500eaeb433eb.jpg', 'screenshot_1697612187-4490428_jpg.rf.7acceea23e3fee7ee953cf0d8fe90853.jpg']\n"]}]},{"cell_type":"code","source":["# Count number of training images\n","if os.path.exists(TRAIN_IMAGES_PATH):\n","    train_image_files_list = os.listdir(TRAIN_IMAGES_PATH)\n","    number_train_files = len(train_image_files_list)\n","    print(f\"Number of training images: {number_train_files}\")\n","else:\n","    print(f\"Cannot count training images, path not found: {TRAIN_IMAGES_PATH}\")\n","    number_train_files = 0"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gt6TBKnOiXb8","executionInfo":{"status":"ok","timestamp":1748857663672,"user_tz":-420,"elapsed":19,"user":{"displayName":"Samuel Ady Sanjaya","userId":"15801194796254517624"}},"outputId":"1e1fab2c-0fc1-4f0f-ce81-a6998fc9fb30"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of training images: 1467\n"]}]},{"cell_type":"code","source":["# --- Define a custom Dataset for SSD ---\n","class CustomObjectDetectionDataset(Dataset):\n","    def __init__(self, img_dir, label_dir, transforms=None):\n","        self.img_dir = img_dir\n","        self.label_dir = label_dir\n","        self.transforms = transforms\n","        self.image_files = [f for f in os.listdir(img_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n","\n","    def __len__(self):\n","        return len(self.image_files)\n","\n","    def __getitem__(self, idx):\n","        img_name = self.image_files[idx]\n","        img_path = os.path.join(self.img_dir, img_name)\n","        label_name = os.path.splitext(img_name)[0] + '.txt'\n","        label_path = os.path.join(self.label_dir, label_name)\n","\n","        img = cv2.imread(img_path)\n","        if img is None:\n","            # Handle unreadable images (e.g., return a placeholder or skip)\n","            print(f\"Warning: Could not read image {img_path}. Skipping this item.\")\n","            return self.__getitem__((idx + 1) % len(self)) # Load next image if current one fails\n","\n","        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","        H, W = img.shape[0], img.shape[1]\n","\n","        boxes = []\n","        labels = [] # Assuming 1 class for 'car'\n","        if os.path.exists(label_path):\n","            with open(label_path, 'r') as f:\n","                for line in f:\n","                    parts = line.strip().split()\n","                    if len(parts) == 5:\n","                        class_id, x_center, y_center, width, height = map(float, parts)\n","                        # Convert YOLO format (normalized center, width, height) to [x_min, y_min, x_max, y_max]\n","                        # SSD usually expects normalized [x_min, y_min, x_max, y_max]\n","                        x_min = (x_center - width / 2) * W\n","                        y_min = (y_center - height / 2) * H\n","                        x_max = (x_center + width / 2) * W\n","                        y_max = (y_center + height / 2) * H\n","                        boxes.append([x_min, y_min, x_max, y_max])\n","                        labels.append(int(class_id) + 1) # Add 1 because SSD usually treats background as class 0\n","\n","        boxes_tensor = torch.as_tensor(boxes, dtype=torch.float32).reshape(-1, 4)\n","        labels_tensor = torch.as_tensor(labels, dtype=torch.int64)\n","\n","        target = {}\n","        target[\"boxes\"] = boxes_tensor\n","        target[\"labels\"] = labels_tensor\n","        target[\"image_id\"] = torch.tensor([idx])\n","        target[\"area\"] = (boxes_tensor[:, 3] - boxes_tensor[:, 1]) * (boxes_tensor[:, 2] - boxes_tensor[:, 0])\n","        target[\"iscrowd\"] = torch.zeros((len(boxes),), dtype=torch.int64)\n","\n","        if self.transforms:\n","            img = self.transforms(img) # Convert to Tensor and normalize\n","\n","        return img, target, img_path # Return img_path for display purposes\n","\n","# --- Helper for plotting ---\n","def draw_boxes_on_image(img_rgb, boxes, labels, class_names=None):\n","    img = img_rgb.copy()\n","    for i, box in enumerate(boxes):\n","        x1, y1, x2, y2 = map(int, box)\n","        cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 2)\n","        if class_names and labels is not None:\n","            label_text = class_names[labels[i]] if labels[i] < len(class_names) else f'Class {labels[i]}'\n","            cv2.putText(img, label_text, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n","    return img\n","\n","# --- Main display logic ---\n","if number_train_files >= 16:\n","    # A simple transform for display: just convert to tensor\n","    display_transforms = torchvision.transforms.Compose([\n","        torchvision.transforms.ToTensor()\n","    ])\n","    train_dataset_display = CustomObjectDetectionDataset(\n","        img_dir=TRAIN_IMAGES_PATH,\n","        label_dir=TRAIN_LABELS_PATH,\n","        transforms=display_transforms\n","    )\n","\n","    # Assuming a simple class map (adjust if you have multiple classes in your data.yaml)\n","    # You'd typically read this from data.yaml, but for a single class, it's simpler.\n","    # If your class_id in labels.txt is 0 for 'car', then class_names = ['background', 'car']\n","    CLASS_NAMES = ['__background__', 'car'] # SSD typically uses 0 for background\n","\n","    fig, axs = plt.subplots(4, 4, figsize=(16, 16))\n","    fig.suptitle('Sample Annotated Training Images (SSD Format)', fontsize=20)\n","\n","    # Get random indices to display\n","    random_indices = random.sample(range(len(train_dataset_display)), 16)\n","\n","    for i, idx in enumerate(random_indices):\n","        img_tensor, target, img_path = train_dataset_display[idx]\n","\n","        # Convert tensor back to numpy for OpenCV drawing\n","        img_np = img_tensor.permute(1, 2, 0).numpy() # CxHxW to HxWxC\n","        img_np = (img_np * 255).astype(np.uint8) # Scale to 0-255\n","\n","        # Draw boxes (target[\"boxes\"] might be normalized, so you might need to scale them)\n","        # The CustomObjectDetectionDataset already outputs pixel coordinates, so no scaling is needed here\n","        annotated_img = draw_boxes_on_image(img_np, target[\"boxes\"].tolist(), target[\"labels\"].tolist(), CLASS_NAMES)\n","\n","        ax = axs[i // 4, i % 4]\n","        ax.imshow(annotated_img)\n","        ax.set_title(os.path.basename(img_path), fontsize=8)\n","        ax.axis('off')\n","\n","    plt.tight_layout(rect=[0, 0, 1, 0.96])\n","    plt.show()\n","elif number_train_files > 0:\n","    print(f\"Not enough training images to display 16 samples. Found {number_train_files}.\")\n","else:\n","    print(\"No training images found to display.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1X3-0D2ux9CdITq1i_RgjYuAMOVRUQVL8"},"id":"1Kqxy1cRiXZT","executionInfo":{"status":"ok","timestamp":1748857673392,"user_tz":-420,"elapsed":9714,"user":{"displayName":"Samuel Ady Sanjaya","userId":"15801194796254517624"}},"outputId":"174a08aa-76e2-4bb7-dcad-96b746462adf"},"execution_count":33,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","source":["!pip install --upgrade torchvision"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M9yDJ2qFkAoQ","executionInfo":{"status":"ok","timestamp":1748857675063,"user_tz":-420,"elapsed":1657,"user":{"displayName":"Samuel Ady Sanjaya","userId":"15801194796254517624"}},"outputId":"81a82187-017d-421b-b73c-8e24b0088f53"},"execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.22.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n","Requirement already satisfied: torch==2.7.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.7.0)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->torchvision) (3.18.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->torchvision) (4.13.2)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->torchvision) (1.14.0)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->torchvision) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->torchvision) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->torchvision) (2025.3.2)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->torchvision) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->torchvision) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->torchvision) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->torchvision) (9.5.1.17)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->torchvision) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->torchvision) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->torchvision) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->torchvision) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->torchvision) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->torchvision) (0.6.3)\n","Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->torchvision) (2.26.2)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->torchvision) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->torchvision) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->torchvision) (1.11.1.6)\n","Requirement already satisfied: triton==3.3.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->torchvision) (3.3.0)\n","Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.3.0->torch==2.7.0->torchvision) (75.2.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch==2.7.0->torchvision) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.7.0->torchvision) (3.0.2)\n"]}]},{"cell_type":"code","source":["# --- Define the SSD Model with MobileNetV3 Backbone ---\n","import torch.nn as nn # Import the neural network module\n","from collections import OrderedDict # Import OrderedDict\n","from torchvision.models.detection.ssd import SSD\n","from torchvision.models.detection.anchor_utils import AnchorGenerator\n","from torchvision.models import mobilenet_v3_large, MobileNet_V3_Large_Weights\n","\n","\n","def create_ssd_mobilenetv3(num_classes):\n","    \"\"\"\n","    Creates an SSD model with a MobileNetV3-Large backbone.\n","\n","    Args:\n","        num_classes (int): The number of object classes (including background).\n","                           For 'car' + background, this would be 2.\n","\n","    Returns:\n","        torchvision.models.detection.SSD: The SSD model.\n","    \"\"\"\n","    # Load MobileNetV3 Large backbone\n","    # We need to adapt it to provide feature maps at multiple scales for SSD\n","\n","    # Load the pretrained MobileNetV3 Large model\n","    backbone = mobilenet_v3_large(weights=MobileNet_V3_Large_Weights.IMAGENET1K_V1)\n","\n","    # Remove the classifier head\n","    backbone = backbone.features\n","\n","    # Define the layers from which to extract features for SSD\n","    # These indices are chosen to provide feature maps at multiple spatial scales\n","    # Examining mobilenet_v3_large.features structure and selecting layers\n","    # that give different strides (approximate strides relative to input 300x300):\n","    # Layer 0: stride 2\n","    # Layer 2: stride 4\n","    # Layer 4: stride 8\n","    # Layer 6: stride 16\n","    # Layer 9: stride 32\n","    # Layer 12: intermediate\n","    # Layer 15: stride 64 (last convolutional layer before avgpool/classifier)\n","\n","    # Let's select a few layers with increasing strides, similar to SSD's requirements.\n","    # The original SSD300 uses features with strides 8, 16, 32, 64, 100, 300.\n","    # We will select layers from MobileNetV3 that are close to these strides.\n","    # Note: The mapping from layer index to exact stride might be complex in MobileNetV3 due to varying block configurations.\n","    # We select indices based on common practice and visual inspection of the layer structure.\n","\n","    # Indices of the layers in backbone.features to use for feature extraction\n","    # The keys will be used as names for the output features\n","    feature_layers_indices = {\n","        'layer4': 4,  # After block 3, approx stride 8\n","        'layer6': 6,  # After block 5, approx stride 16\n","        'layer9': 9,  # After block 8, approx stride 32\n","        'layer12': 12, # After block 11, intermediate\n","        'layer15': 15  # After block 14, approx stride 64\n","    }\n","\n","    # Create a custom feature extractor module\n","    class MobileNetV3FeatureExtractor(nn.Module):\n","        def __init__(self, backbone_features, return_layers):\n","            super().__init__()\n","            self.backbone_features = backbone_features\n","            # Convert string keys to int indices for module access\n","            self.return_layers = {v: k for k, v in return_layers.items()} # map index -> name\n","            # Create a list of sequential modules up to each return layer\n","            self.feature_sequences = nn.ModuleDict()\n","            current_sequence = nn.Sequential()\n","            for i, module in enumerate(self.backbone_features):\n","                current_sequence.add_module(str(i), module) # Add module by its original index\n","                if i in self.return_layers:\n","                    # Clone the current sequence up to this layer\n","                    self.feature_sequences[str(i)] = current_sequence # Store sequence, key is the index as string\n","                    current_sequence = nn.Sequential() # Start a new sequence for subsequent layers\n","\n","        def forward(self, x):\n","            out = OrderedDict()\n","            temp_x = x\n","            # Need to iterate through the backbone layers sequentially to get intermediate outputs\n","            # and extract features at the specified indices.\n","            current_sequence = nn.Sequential()\n","            for i, module in enumerate(self.backbone_features):\n","                temp_x = module(temp_x)\n","                if i in self.return_layers:\n","                    # Store the output with the corresponding name\n","                    out[self.return_layers[i]] = temp_x\n","            return out\n","\n","\n","    # Create the feature extractor instance\n","    feature_extractor = MobileNetV3FeatureExtractor(backbone, feature_layers_indices)\n","\n","\n","    # Determine the output channels for each selected layer by passing a dummy tensor\n","    dummy_input = torch.randn(1, 3, 300, 300) # SSD typically uses 300x300 input\n","    dummy_output_features = feature_extractor(dummy_input)\n","\n","\n","    # Get the output channels from the dummy output\n","    out_channels_list = [v.shape[1] for v in dummy_output_features.values()]\n","    print(f\"Feature extraction output channels for SSDHead: {out_channels_list}\")\n","\n","\n","    # Define the anchor generator\n","    # The aspect ratios and sizes need to be carefully chosen based on the dataset and selected feature maps.\n","    # We need an aspect ratio tuple for each feature map layer we are using.\n","    # We used 5 feature maps (corresponding to keys 'layer4' to 'layer15')\n","\n","    # Aspect ratios per feature map layer (list of tuples)\n","    # Example: Use same aspect ratios for all 5 feature maps\n","    # You might need different aspect ratios for different layers based on the scales.\n","    aspect_ratios = ((1.0, 2.0, 0.5),) * len(feature_layers_indices)\n","\n","    # Anchor sizes per feature map layer (list of tuples of ints)\n","    # These should roughly correspond to the object sizes relative to the image size.\n","    # For SSD300, sizes are often based on a linear or exponential scale from 0.2 to 0.9 or 1.0 relative to image size.\n","    # Let's estimate sizes for our 5 layers. These numbers are highly dependent on the dataset.\n","    # You might need to tune these based on performance.\n","    sizes = ((30,), (60,), (111,), (162,), (213,)) # Sizes for 5 layers\n","\n","    anchor_generator = AnchorGenerator(\n","        sizes=sizes,\n","        aspect_ratios=aspect_ratios\n","    )\n","\n","    # Determine the number of anchors per location for each feature map\n","    num_anchors_per_location = anchor_generator.num_anchors_per_location()\n","    print(f\"Number of anchors per location for each feature map: {num_anchors_per_location}\")\n","\n","\n","    # Create the SSD model\n","    # The SSD model combines the backbone (feature extractor), anchor generator, and SSDHead\n","    # The torchvision SSD class automatically creates the SSDHead based on backbone output channels and num_classes.\n","    model = SSD(\n","        backbone=feature_extractor, # Pass the custom feature extractor\n","        num_classes=num_classes,\n","        anchor_generator=anchor_generator,\n","        size=(300,300),\n","        # By default, the internal SSDHead will be created using out_channels_list\n","    )\n","\n","    # You might want to load pretrained weights if available, but not for the head\n","    # mobilenet_v3_large weights are for classification, not detection\n","    # You could potentially load pretrained weights for the backbone features if they exist for detection tasks\n","\n","    return model\n","\n","# --- Configuration ---\n","# Assume 'car' is the only object class + background\n","NUM_CLASSES = 2 # 1 class ('car') + 1 background class\n","\n","# --- Create the Model ---\n","# The model expects input images to be normalized and potentially resized.\n","# torchvision's detection models often handle image resizing implicitly during training/inference,\n","# but it's good practice to handle it in transforms.\n","# For SSD, input size is typically fixed (e.g., 300x300).\n","\n","model = create_ssd_mobilenetv3(NUM_CLASSES)\n","\n","# --- Example Usage (Optional) ---\n","# Print model summary (can be verbose)\n","# print(model)\n","\n","# Move model to GPU if available\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(f\"Using device: {device}\")\n","model.to(device)\n","\n","# You can pass a dummy tensor to check the model output structure\n","# model.eval() # Set model to evaluation mode\n","# with torch.no_grad():\n","#     dummy_input_tensor = torch.randn(1, 3, 300, 300).to(device)\n","#     # Detection models return a list of dictionaries, one per image\n","#     # Each dictionary contains 'boxes', 'labels', 'scores'\n","#     predictions = model([dummy_input_tensor])\n","#     print(\"\\nDummy prediction output structure:\")\n","#     print(predictions)\n","\n","print(\"\\nSSD model with MobileNetV3 backbone created.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rM70CMy9iXBa","executionInfo":{"status":"ok","timestamp":1748857675364,"user_tz":-420,"elapsed":282,"user":{"displayName":"Samuel Ady Sanjaya","userId":"15801194796254517624"}},"outputId":"eea3d2ce-09ce-4ad7-b084-75f0c3461179"},"execution_count":35,"outputs":[{"output_type":"stream","name":"stdout","text":["Feature extraction output channels for SSDHead: [40, 40, 80, 112, 160]\n","Number of anchors per location for each feature map: [3, 3, 3, 3, 3]\n","Using device: cuda\n","\n","SSD model with MobileNetV3 backbone created.\n"]}]},{"cell_type":"code","source":["\n","from torchvision.models.detection.rpn import AnchorGenerator\n","import torch.optim as optim\n","\n","# --- Data Loading for Training ---\n","\n","# Transforms for training data (with data augmentation)\n","# These should include resizing to 300x300 and converting to tensor\n","train_transforms = torchvision.transforms.Compose([\n","    torchvision.transforms.ToPILImage(), # Convert numpy array to PIL Image\n","    torchvision.transforms.Resize((300, 300)), # Resize to SSD input size\n","    torchvision.transforms.ToTensor(), # Convert PIL Image to Tensor (scales to [0, 1])\n","    # Optional: Add data augmentation like horizontal flip, color jitter, etc.\n","    # torchvision.transforms.RandomHorizontalFlip(0.5),\n","    # torchvision.transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n","])\n","\n","# Transforms for validation data (without augmentation, just resizing and tensor conversion)\n","val_transforms = torchvision.transforms.Compose([\n","    torchvision.transforms.ToPILImage(),\n","    torchvision.transforms.Resize((300, 300)),\n","    torchvision.transforms.ToTensor(),\n","])\n","\n","\n","# Create Datasets\n","train_dataset = CustomObjectDetectionDataset(\n","    img_dir=TRAIN_IMAGES_PATH,\n","    label_dir=TRAIN_LABELS_PATH,\n","    transforms=train_transforms\n",")\n","\n","valid_dataset = CustomObjectDetectionDataset(\n","    img_dir=VALID_IMAGES_PATH,\n","    label_dir=VALID_LABELS_PATH,\n","    transforms=val_transforms\n",")\n","\n","# Collate function for detection datasets\n","def collate_fn(batch):\n","    return tuple(zip(*batch))\n","\n","# Create DataLoaders\n","train_loader = DataLoader(\n","    train_dataset,\n","    batch_size=16, # Adjust batch size based on GPU memory\n","    shuffle=True,\n","    num_workers=2, # Adjust based on your system and available RAM/CPU cores\n","    collate_fn=collate_fn\n",")\n","\n","valid_loader = DataLoader(\n","    valid_dataset,\n","    batch_size=16,\n","    shuffle=False, # No need to shuffle validation data\n","    num_workers=2,\n","    collate_fn=collate_fn\n",")\n","\n","print(f\"Training dataset size: {len(train_dataset)}\")\n","print(f\"Validation dataset size: {len(valid_dataset)}\")\n","\n","\n","# --- Optimizer ---\n","# SGD is commonly used for detection models\n","params = [p for p in model.parameters() if p.requires_grad]\n","optimizer = optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n","\n","# Optional: Learning Rate Scheduler (reduces learning rate during training)\n","# lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n","\n","\n","# --- Training Function ---\n","def train_one_epoch(model, optimizer, data_loader, device, epoch):\n","    model.train() # Set model to training mode\n","    total_loss = 0\n","    print(f\"\\nEpoch {epoch}: Training...\")\n","    # Use a progress bar for better visualization (optional)\n","    # from tqdm.auto import tqdm\n","    # data_loader = tqdm(data_loader, desc=f\"Epoch {epoch}\")\n","\n","    for i, (images, targets, _) in enumerate(data_loader):\n","        images = list(image.to(device) for image in images)\n","        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n","\n","        # Forward pass\n","        # In training mode, detection models return a dictionary of losses\n","        loss_dict = model(images, targets)\n","        losses = sum(loss for loss in loss_dict.values())\n","\n","        # Backward pass and optimize\n","        optimizer.zero_grad()\n","        # Add gradient clipping if needed\n","        # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.1)\n","        losses.backward()\n","        optimizer.step()\n","\n","        total_loss += losses.item()\n","\n","        # Print loss every few batches\n","        if (i + 1) % 10 == 0: # Print every 10 batches\n","            print(f\"  Batch {i+1}/{len(data_loader)}, Loss: {losses.item():.4f}\")\n","\n","    avg_loss = total_loss / len(data_loader)\n","    print(f\"Epoch {epoch} finished. Average Loss: {avg_loss:.4f}\")\n","    return avg_loss\n","\n","# --- Evaluation Function (Modified for collecting predictions) ---\n","# This function collects predictions and ground truth for proper evaluation (e.g., mAP)\n","def evaluate_model(model, data_loader, device):\n","    model.eval() # Set model to evaluation mode\n","    # total_loss = 0 # Loss calculation is removed from evaluation\n","    print(\"\\nEvaluating (Collecting Predictions)...\")\n","    predictions = []\n","    ground_truths = []\n","\n","    # Use a progress bar for better visualization (optional)\n","    # from tqdm.auto import tqdm\n","    # data_loader = tqdm(data_loader, desc=\"Evaluating\")\n","\n","    with torch.no_grad(): # No need to calculate gradients during evaluation\n","        for i, (images, targets, _) in enumerate(data_loader):\n","            images = list(image.to(device) for image in images)\n","            # targets = [{k: v.to(device) for k, v in t.items()} for t in targets] # Targets are ground truth\n","\n","            # Forward pass - in eval mode, model returns a list of dictionaries (predictions)\n","            outputs = model(images)\n","\n","            predictions.extend(outputs) # Collect predictions\n","            ground_truths.extend(targets) # Collect corresponding ground truths\n","\n","            # # Original loss calculation (removed)\n","            # loss_dict = model(images, targets) # This will raise error in eval mode\n","            # losses = sum(loss for loss in loss_dict.values())\n","            # total_loss += losses.item()\n","\n","    # avg_loss = total_loss / len(data_loader) # Loss calculation removed\n","    # print(f\"Evaluation finished. Average Loss: {avg_loss:.4f}\")\n","\n","    print(f\"Finished collecting predictions and ground truths for {len(predictions)} images.\")\n","    # TODO: Implement mAP calculation here using collected predictions and ground_truths\n","    # You will need a library like torchmetrics or pycocotools for this.\n","    # Example using torchmetrics (install with: pip install torchmetrics[detection])\n","    # from torchmetrics.detection.mean_ap import MeanAveragePrecision\n","    # metric = MeanAveragePrecision()\n","    # metric.update(predictions, ground_truths)\n","    # map_result = metric.compute()\n","    # print(f\"mAP results: {map_result}\")\n","\n","    # Returning None for loss as we are not calculating it here\n","    return None # Return collected predictions and ground_truths if needed for external mAP calculation\n","\n","# --- Training Loop ---\n","num_epochs = 50 # As requested\n","# Store only training loss, as validation loss calculation is removed\n","history = {'train_loss': []} #'val_loss': []}\n","\n","# Create directory for saving model weights if it doesn't exist\n","model_weights_dir = os.path.join(FULL_EXPERIMENT_PATH, 'weights')\n","os.makedirs(model_weights_dir, exist_ok=True)\n","\n","# Removed best_val_loss and saving based on it, as we are not calculating validation loss here.\n","# best_val_loss = float('inf') # Initialize with a high value\n","\n","for epoch in range(num_epochs):\n","    # Train for one epoch\n","    train_loss = train_one_epoch(model, optimizer, train_loader, device, epoch)\n","    history['train_loss'].append(train_loss)\n","\n","    # Evaluate on the validation set (now only collects predictions, does not calculate loss)\n","    # A proper evaluation (mAP) would be done here after collecting predictions\n","    # val_loss = evaluate_model(model, valid_loader, device)\n","    # history['val_loss'].append(val_loss) # Removed adding to history\n","\n","    # Step the learning rate scheduler (if using one)\n","    # if lr_scheduler is not None:\n","    #     lr_scheduler.step()\n","\n","    # Removed saving the best model based on validation loss\n","\n","    # Save the latest model state after each epoch\n","    last_model_path = os.path.join(model_weights_dir, f'epoch_{epoch+1}_last.pt')\n","    torch.save(model.state_dict(), last_model_path)\n","    print(f\"Saved model after epoch {epoch+1} to {last_model_path}\")\n","\n","# Optional: Save the final model after training\n","final_model_path = os.path.join(model_weights_dir, 'final.pt')\n","torch.save(model.state_dict(), final_model_path)\n","print(f\"\\nTraining completed! Final model saved to {final_model_path}\")\n","\n","\n","# --- Plot Training History ---\n","plt.figure(figsize=(10, 6))\n","plt.plot(history['train_loss'], label='Training Loss')\n","# plt.plot(history['val_loss'], label='Validation Loss') # Removed plotting validation loss\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.title('Training Loss per Epoch') # Updated title\n","plt.legend()\n","plt.grid(True)\n","plt.show()\n","\n","# You can also save the history\n","history_path = os.path.join(FULL_EXPERIMENT_PATH, 'training_history.json')\n","with open(history_path, 'w') as f:\n","    json.dump(history, f)\n","print(f\"Training history saved to {history_path}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"gI14cZ1svQ7a","executionInfo":{"status":"ok","timestamp":1748858638618,"user_tz":-420,"elapsed":740760,"user":{"displayName":"Samuel Ady Sanjaya","userId":"15801194796254517624"}},"outputId":"986aa964-ffb4-4845-b383-96d0ef2b648d"},"execution_count":38,"outputs":[{"output_type":"stream","name":"stdout","text":["Training dataset size: 1467\n","Validation dataset size: 233\n","\n","Epoch 0: Training...\n","  Batch 10/92, Loss: 6.7673\n","  Batch 20/92, Loss: 5.4328\n","  Batch 30/92, Loss: 6.0078\n","  Batch 40/92, Loss: 5.3481\n","  Batch 50/92, Loss: 5.0424\n","  Batch 60/92, Loss: 4.8402\n","  Batch 70/92, Loss: 5.7627\n","  Batch 80/92, Loss: 5.2692\n","  Batch 90/92, Loss: 7.2789\n","Epoch 0 finished. Average Loss: 6.1280\n","Saved model after epoch 1 to /content/drive/MyDrive/Projects/Car Detection v2/Colab Env/Model History/singleclass_colab_YOLOv8/weights/epoch_1_last.pt\n","\n","Epoch 1: Training...\n","  Batch 10/92, Loss: 5.5967\n","  Batch 20/92, Loss: 5.3746\n","  Batch 30/92, Loss: 7.3763\n","  Batch 40/92, Loss: 4.6983\n","  Batch 50/92, Loss: 9.5907\n","  Batch 60/92, Loss: 5.2608\n","  Batch 70/92, Loss: 6.4460\n","  Batch 80/92, Loss: 5.1955\n","  Batch 90/92, Loss: 7.3430\n","Epoch 1 finished. Average Loss: 6.0708\n","Saved model after epoch 2 to /content/drive/MyDrive/Projects/Car Detection v2/Colab Env/Model History/singleclass_colab_YOLOv8/weights/epoch_2_last.pt\n","\n","Epoch 2: Training...\n","  Batch 10/92, Loss: 5.1759\n","  Batch 20/92, Loss: 5.5248\n","  Batch 30/92, Loss: 5.9131\n","  Batch 40/92, Loss: 7.7963\n","  Batch 50/92, Loss: 6.7999\n","  Batch 60/92, Loss: 5.2843\n","  Batch 70/92, Loss: 5.7932\n","  Batch 80/92, Loss: 5.6922\n","  Batch 90/92, Loss: 5.6874\n","Epoch 2 finished. Average Loss: 6.0179\n","Saved model after epoch 3 to /content/drive/MyDrive/Projects/Car Detection v2/Colab Env/Model History/singleclass_colab_YOLOv8/weights/epoch_3_last.pt\n","\n","Epoch 3: Training...\n","  Batch 10/92, Loss: 6.3653\n","  Batch 20/92, Loss: 4.6647\n","  Batch 30/92, Loss: 6.7012\n","  Batch 40/92, Loss: 8.5115\n","  Batch 50/92, Loss: 4.4898\n","  Batch 60/92, Loss: 6.2633\n","  Batch 70/92, Loss: 5.3845\n","  Batch 80/92, Loss: 5.5593\n","  Batch 90/92, Loss: 6.1685\n","Epoch 3 finished. Average Loss: 5.9698\n","Saved model after epoch 4 to /content/drive/MyDrive/Projects/Car Detection v2/Colab Env/Model History/singleclass_colab_YOLOv8/weights/epoch_4_last.pt\n","\n","Epoch 4: Training...\n","  Batch 10/92, Loss: 5.1053\n","  Batch 20/92, Loss: 5.1023\n","  Batch 30/92, Loss: 5.4549\n","  Batch 40/92, Loss: 6.3434\n","  Batch 50/92, Loss: 5.5049\n","  Batch 60/92, Loss: 5.1832\n","  Batch 70/92, Loss: 6.1149\n","  Batch 80/92, Loss: 5.6872\n","  Batch 90/92, Loss: 5.1325\n","Epoch 4 finished. Average Loss: 5.9028\n","Saved model after epoch 5 to /content/drive/MyDrive/Projects/Car Detection v2/Colab Env/Model History/singleclass_colab_YOLOv8/weights/epoch_5_last.pt\n","\n","Epoch 5: Training...\n","  Batch 10/92, Loss: 4.8619\n","  Batch 20/92, Loss: 4.9516\n","  Batch 30/92, Loss: 4.9590\n","  Batch 40/92, Loss: 5.3764\n","  Batch 50/92, Loss: 6.0245\n","  Batch 60/92, Loss: 5.3796\n","  Batch 70/92, Loss: 4.6740\n","  Batch 80/92, Loss: 6.6975\n","  Batch 90/92, Loss: 6.8168\n","Epoch 5 finished. Average Loss: 5.8593\n","Saved model after epoch 6 to /content/drive/MyDrive/Projects/Car Detection v2/Colab Env/Model History/singleclass_colab_YOLOv8/weights/epoch_6_last.pt\n","\n","Epoch 6: Training...\n","  Batch 10/92, Loss: 4.3669\n","  Batch 20/92, Loss: 5.4553\n","  Batch 30/92, Loss: 5.2888\n","  Batch 40/92, Loss: 8.0376\n","  Batch 50/92, Loss: 5.0042\n","  Batch 60/92, Loss: 6.6676\n","  Batch 70/92, Loss: 6.8461\n","  Batch 80/92, Loss: 5.2186\n","  Batch 90/92, Loss: 6.6064\n","Epoch 6 finished. Average Loss: 5.7999\n","Saved model after epoch 7 to /content/drive/MyDrive/Projects/Car Detection v2/Colab Env/Model History/singleclass_colab_YOLOv8/weights/epoch_7_last.pt\n","\n","Epoch 7: Training...\n","  Batch 10/92, Loss: 4.8006\n","  Batch 20/92, Loss: 6.0144\n","  Batch 30/92, Loss: 5.1886\n","  Batch 40/92, Loss: 6.8062\n","  Batch 50/92, Loss: 5.8918\n","  Batch 60/92, Loss: 5.5512\n","  Batch 70/92, Loss: 5.0690\n","  Batch 80/92, Loss: 4.8141\n","  Batch 90/92, Loss: 5.9475\n","Epoch 7 finished. Average Loss: 5.7394\n","Saved model after epoch 8 to /content/drive/MyDrive/Projects/Car Detection v2/Colab Env/Model History/singleclass_colab_YOLOv8/weights/epoch_8_last.pt\n","\n","Epoch 8: Training...\n","  Batch 10/92, Loss: 5.7647\n","  Batch 20/92, Loss: 5.1452\n","  Batch 30/92, Loss: 6.5093\n","  Batch 40/92, Loss: 5.3440\n","  Batch 50/92, Loss: 4.7743\n","  Batch 60/92, Loss: 5.9339\n","  Batch 70/92, Loss: 7.3852\n","  Batch 80/92, Loss: 5.9473\n","  Batch 90/92, Loss: 5.2506\n","Epoch 8 finished. Average Loss: 5.6971\n","Saved model after epoch 9 to /content/drive/MyDrive/Projects/Car Detection v2/Colab Env/Model History/singleclass_colab_YOLOv8/weights/epoch_9_last.pt\n","\n","Epoch 9: Training...\n","  Batch 10/92, Loss: 5.8213\n","  Batch 20/92, Loss: 5.2194\n","  Batch 30/92, Loss: 5.2411\n","  Batch 40/92, Loss: 5.3826\n","  Batch 50/92, Loss: 5.1784\n","  Batch 60/92, Loss: 4.4972\n","  Batch 70/92, Loss: 5.4582\n","  Batch 80/92, Loss: 9.0483\n","  Batch 90/92, Loss: 10.9566\n","Epoch 9 finished. Average Loss: 5.7167\n","Saved model after epoch 10 to /content/drive/MyDrive/Projects/Car Detection v2/Colab Env/Model History/singleclass_colab_YOLOv8/weights/epoch_10_last.pt\n","\n","Epoch 10: Training...\n","  Batch 10/92, Loss: 6.3110\n","  Batch 20/92, Loss: 4.8511\n","  Batch 30/92, Loss: 8.9087\n","  Batch 40/92, Loss: 5.1246\n","  Batch 50/92, Loss: 5.3229\n","  Batch 60/92, Loss: 4.6782\n","  Batch 70/92, Loss: 6.3667\n","  Batch 80/92, Loss: 5.9978\n","  Batch 90/92, Loss: 5.2142\n","Epoch 10 finished. Average Loss: 5.6514\n","Saved model after epoch 11 to /content/drive/MyDrive/Projects/Car Detection v2/Colab Env/Model History/singleclass_colab_YOLOv8/weights/epoch_11_last.pt\n","\n","Epoch 11: Training...\n","  Batch 10/92, Loss: 5.4059\n","  Batch 20/92, Loss: 5.7683\n","  Batch 30/92, Loss: 4.9932\n","  Batch 40/92, Loss: 6.3462\n","  Batch 50/92, Loss: 5.9241\n","  Batch 60/92, Loss: 5.1516\n","  Batch 70/92, Loss: 5.6887\n","  Batch 80/92, Loss: 4.6837\n","  Batch 90/92, Loss: 4.7361\n","Epoch 11 finished. Average Loss: 5.5694\n","Saved model after epoch 12 to /content/drive/MyDrive/Projects/Car Detection v2/Colab Env/Model History/singleclass_colab_YOLOv8/weights/epoch_12_last.pt\n","\n","Epoch 12: Training...\n","  Batch 10/92, Loss: 4.1109\n","  Batch 20/92, Loss: 3.9749\n","  Batch 30/92, Loss: 5.6694\n","  Batch 40/92, Loss: 5.3521\n","  Batch 50/92, Loss: 5.0721\n","  Batch 60/92, Loss: 6.9902\n","  Batch 70/92, Loss: 5.9859\n","  Batch 80/92, Loss: 7.0159\n","  Batch 90/92, Loss: 7.0280\n","Epoch 12 finished. Average Loss: 5.5479\n","Saved model after epoch 13 to /content/drive/MyDrive/Projects/Car Detection v2/Colab Env/Model History/singleclass_colab_YOLOv8/weights/epoch_13_last.pt\n","\n","Epoch 13: Training...\n","  Batch 10/92, Loss: 4.4962\n","  Batch 20/92, Loss: 4.2675\n","  Batch 30/92, Loss: 5.8043\n","  Batch 40/92, Loss: 4.9633\n","  Batch 50/92, Loss: 4.5031\n","  Batch 60/92, Loss: 4.8992\n","  Batch 70/92, Loss: 4.7961\n","  Batch 80/92, Loss: 6.3844\n","  Batch 90/92, Loss: 5.6470\n","Epoch 13 finished. Average Loss: 5.4587\n","Saved model after epoch 14 to /content/drive/MyDrive/Projects/Car Detection v2/Colab Env/Model History/singleclass_colab_YOLOv8/weights/epoch_14_last.pt\n","\n","Epoch 14: Training...\n","  Batch 10/92, Loss: 5.2994\n","  Batch 20/92, Loss: 5.1100\n","  Batch 30/92, Loss: 5.5537\n","  Batch 40/92, Loss: 8.4660\n","  Batch 50/92, Loss: 5.3864\n","  Batch 60/92, Loss: 5.3948\n","  Batch 70/92, Loss: 5.2000\n","  Batch 80/92, Loss: 5.0034\n","  Batch 90/92, Loss: 8.7805\n","Epoch 14 finished. Average Loss: 5.4089\n","Saved model after epoch 15 to /content/drive/MyDrive/Projects/Car Detection v2/Colab Env/Model History/singleclass_colab_YOLOv8/weights/epoch_15_last.pt\n","\n","Epoch 15: Training...\n","  Batch 10/92, Loss: 4.5685\n","  Batch 20/92, Loss: 5.4148\n","  Batch 30/92, Loss: 4.4583\n","  Batch 40/92, Loss: 4.8443\n","  Batch 50/92, Loss: 5.1442\n","  Batch 60/92, Loss: 6.0469\n","  Batch 70/92, Loss: 5.3317\n","  Batch 80/92, Loss: 5.9852\n","  Batch 90/92, Loss: 6.2262\n","Epoch 15 finished. Average Loss: 5.3120\n","Saved model after epoch 16 to /content/drive/MyDrive/Projects/Car Detection v2/Colab Env/Model History/singleclass_colab_YOLOv8/weights/epoch_16_last.pt\n","\n","Epoch 16: Training...\n","  Batch 10/92, Loss: 4.5421\n","  Batch 20/92, Loss: 4.5932\n","  Batch 30/92, Loss: 4.9883\n","  Batch 40/92, Loss: 4.6277\n","  Batch 50/92, Loss: 5.6103\n","  Batch 60/92, Loss: 5.8590\n","  Batch 70/92, Loss: 6.0945\n","  Batch 80/92, Loss: 6.3752\n","  Batch 90/92, Loss: 5.3766\n","Epoch 16 finished. Average Loss: 5.3676\n","Saved model after epoch 17 to /content/drive/MyDrive/Projects/Car Detection v2/Colab Env/Model History/singleclass_colab_YOLOv8/weights/epoch_17_last.pt\n","\n","Epoch 17: Training...\n","  Batch 10/92, Loss: 5.7264\n","  Batch 20/92, Loss: 4.2681\n","  Batch 30/92, Loss: 4.5054\n","  Batch 40/92, Loss: 5.7136\n","  Batch 50/92, Loss: 4.6981\n","  Batch 60/92, Loss: 4.9099\n","  Batch 70/92, Loss: 4.8203\n","  Batch 80/92, Loss: 4.4055\n","  Batch 90/92, Loss: 4.1192\n","Epoch 17 finished. Average Loss: 5.2775\n","Saved model after epoch 18 to /content/drive/MyDrive/Projects/Car Detection v2/Colab Env/Model History/singleclass_colab_YOLOv8/weights/epoch_18_last.pt\n","\n","Epoch 18: Training...\n","  Batch 10/92, Loss: 4.8923\n","  Batch 20/92, Loss: 4.9427\n","  Batch 30/92, Loss: 4.4606\n","  Batch 40/92, Loss: 4.5043\n","  Batch 50/92, Loss: 4.9668\n","  Batch 60/92, Loss: 5.5212\n","  Batch 70/92, Loss: 6.1360\n","  Batch 80/92, Loss: 4.4454\n","  Batch 90/92, Loss: 5.0053\n","Epoch 18 finished. Average Loss: 5.1538\n","Saved model after epoch 19 to /content/drive/MyDrive/Projects/Car Detection v2/Colab Env/Model History/singleclass_colab_YOLOv8/weights/epoch_19_last.pt\n","\n","Epoch 19: Training...\n","  Batch 10/92, Loss: 4.6140\n","  Batch 20/92, Loss: 6.3159\n","  Batch 30/92, Loss: 4.7217\n","  Batch 40/92, Loss: 4.4623\n","  Batch 50/92, Loss: 3.9537\n","  Batch 60/92, Loss: 4.6880\n","  Batch 70/92, Loss: 5.7223\n","  Batch 80/92, Loss: 4.6279\n","  Batch 90/92, Loss: 5.3516\n","Epoch 19 finished. Average Loss: 5.1341\n","Saved model after epoch 20 to /content/drive/MyDrive/Projects/Car Detection v2/Colab Env/Model History/singleclass_colab_YOLOv8/weights/epoch_20_last.pt\n","\n","Epoch 20: Training...\n","  Batch 10/92, Loss: 5.2597\n","  Batch 20/92, Loss: 4.5143\n","  Batch 30/92, Loss: 4.2608\n","  Batch 40/92, Loss: 5.1035\n","  Batch 50/92, Loss: 5.0861\n","  Batch 60/92, Loss: 4.1750\n","  Batch 70/92, Loss: 4.8695\n","  Batch 80/92, Loss: 6.2360\n","  Batch 90/92, Loss: 4.4737\n","Epoch 20 finished. Average Loss: 5.1584\n","Saved model after epoch 21 to /content/drive/MyDrive/Projects/Car Detection v2/Colab Env/Model History/singleclass_colab_YOLOv8/weights/epoch_21_last.pt\n","\n","Epoch 21: Training...\n","  Batch 10/92, Loss: 4.7898\n","  Batch 20/92, Loss: 4.7361\n","  Batch 30/92, Loss: 5.9372\n","  Batch 40/92, Loss: 5.1840\n","  Batch 50/92, Loss: 3.9637\n","  Batch 60/92, Loss: 6.2905\n","  Batch 70/92, Loss: 4.5806\n","  Batch 80/92, Loss: 5.6647\n","  Batch 90/92, Loss: 6.0995\n","Epoch 21 finished. Average Loss: 5.0909\n","Saved model after epoch 22 to /content/drive/MyDrive/Projects/Car Detection v2/Colab Env/Model History/singleclass_colab_YOLOv8/weights/epoch_22_last.pt\n","\n","Epoch 22: Training...\n","  Batch 10/92, Loss: 4.6896\n","  Batch 20/92, Loss: 4.7846\n","  Batch 30/92, Loss: 5.3002\n","  Batch 40/92, Loss: 5.0875\n","  Batch 50/92, Loss: 5.4780\n","  Batch 60/92, Loss: 4.6319\n","  Batch 70/92, Loss: 4.5780\n","  Batch 80/92, Loss: 4.0336\n","  Batch 90/92, Loss: 4.3344\n","Epoch 22 finished. Average Loss: 5.0344\n","Saved model after epoch 23 to /content/drive/MyDrive/Projects/Car Detection v2/Colab Env/Model History/singleclass_colab_YOLOv8/weights/epoch_23_last.pt\n","\n","Epoch 23: Training...\n","  Batch 10/92, Loss: 5.7815\n","  Batch 20/92, Loss: 3.8501\n","  Batch 30/92, Loss: 4.8331\n","  Batch 40/92, Loss: 5.5143\n","  Batch 50/92, Loss: 5.2385\n","  Batch 60/92, Loss: 5.2626\n","  Batch 70/92, Loss: 3.4236\n","  Batch 80/92, Loss: 4.7617\n","  Batch 90/92, Loss: 4.2553\n","Epoch 23 finished. Average Loss: 5.0158\n","Saved model after epoch 24 to /content/drive/MyDrive/Projects/Car Detection v2/Colab Env/Model History/singleclass_colab_YOLOv8/weights/epoch_24_last.pt\n","\n","Epoch 24: Training...\n","  Batch 10/92, Loss: 3.9061\n","  Batch 20/92, Loss: 4.7765\n","  Batch 30/92, Loss: 5.1056\n","  Batch 40/92, Loss: 4.4507\n","  Batch 50/92, Loss: 5.3149\n","  Batch 60/92, Loss: 4.6485\n","  Batch 70/92, Loss: 3.9103\n","  Batch 80/92, Loss: 4.2395\n","  Batch 90/92, Loss: 7.7424\n","Epoch 24 finished. Average Loss: 5.1571\n","Saved model after epoch 25 to /content/drive/MyDrive/Projects/Car Detection v2/Colab Env/Model History/singleclass_colab_YOLOv8/weights/epoch_25_last.pt\n","\n","Epoch 25: Training...\n","  Batch 10/92, Loss: 4.0563\n","  Batch 20/92, Loss: 4.3205\n","  Batch 30/92, Loss: 5.5833\n","  Batch 40/92, Loss: 6.0094\n","  Batch 50/92, Loss: 3.9673\n","  Batch 60/92, Loss: 5.8095\n","  Batch 70/92, Loss: 4.0966\n","  Batch 80/92, Loss: 6.1065\n","  Batch 90/92, Loss: 4.7181\n","Epoch 25 finished. Average Loss: 4.9751\n","Saved model after epoch 26 to /content/drive/MyDrive/Projects/Car Detection v2/Colab Env/Model History/singleclass_colab_YOLOv8/weights/epoch_26_last.pt\n","\n","Epoch 26: Training...\n","  Batch 10/92, Loss: 5.5750\n","  Batch 20/92, Loss: 5.4913\n","  Batch 30/92, Loss: 5.3290\n","  Batch 40/92, Loss: 5.4975\n","  Batch 50/92, Loss: 4.1915\n","  Batch 60/92, Loss: 4.3419\n","  Batch 70/92, Loss: 4.1346\n","  Batch 80/92, Loss: 5.0136\n","  Batch 90/92, Loss: 4.6268\n","Epoch 26 finished. Average Loss: 4.9869\n","Saved model after epoch 27 to /content/drive/MyDrive/Projects/Car Detection v2/Colab Env/Model History/singleclass_colab_YOLOv8/weights/epoch_27_last.pt\n","\n","Epoch 27: Training...\n","  Batch 10/92, Loss: 5.9689\n","  Batch 20/92, Loss: 4.9479\n","  Batch 30/92, Loss: 5.3530\n","  Batch 40/92, Loss: 4.8619\n","  Batch 50/92, Loss: 5.1041\n","  Batch 60/92, Loss: 4.8904\n","  Batch 70/92, Loss: 4.8168\n","  Batch 80/92, Loss: 4.8659\n","  Batch 90/92, Loss: 5.3232\n","Epoch 27 finished. Average Loss: 4.9738\n","Saved model after epoch 28 to /content/drive/MyDrive/Projects/Car Detection v2/Colab Env/Model History/singleclass_colab_YOLOv8/weights/epoch_28_last.pt\n","\n","Epoch 28: Training...\n","  Batch 10/92, Loss: 4.6252\n","  Batch 20/92, Loss: 4.0468\n","  Batch 30/92, Loss: 3.5078\n","  Batch 40/92, Loss: 5.7614\n","  Batch 50/92, Loss: 4.3648\n","  Batch 60/92, Loss: 5.8234\n","  Batch 70/92, Loss: 5.3762\n","  Batch 80/92, Loss: 6.4455\n","  Batch 90/92, Loss: 4.5280\n","Epoch 28 finished. Average Loss: 4.9393\n","Saved model after epoch 29 to /content/drive/MyDrive/Projects/Car Detection v2/Colab Env/Model History/singleclass_colab_YOLOv8/weights/epoch_29_last.pt\n","\n","Epoch 29: Training...\n","  Batch 10/92, Loss: 6.0453\n","  Batch 20/92, Loss: 6.7751\n","  Batch 30/92, Loss: 5.6489\n","  Batch 40/92, Loss: 6.1140\n","  Batch 50/92, Loss: 5.0390\n","  Batch 60/92, Loss: 7.0364\n","  Batch 70/92, Loss: 4.1822\n","  Batch 80/92, Loss: 4.3452\n","  Batch 90/92, Loss: 4.9820\n","Epoch 29 finished. Average Loss: 4.8957\n","Saved model after epoch 30 to /content/drive/MyDrive/Projects/Car Detection v2/Colab Env/Model History/singleclass_colab_YOLOv8/weights/epoch_30_last.pt\n","\n","Epoch 30: Training...\n","  Batch 10/92, Loss: 4.3651\n","  Batch 20/92, Loss: 4.6875\n","  Batch 30/92, Loss: 3.5162\n","  Batch 40/92, Loss: 4.5934\n","  Batch 50/92, Loss: 4.6709\n","  Batch 60/92, Loss: 5.2277\n","  Batch 70/92, Loss: 5.3620\n","  Batch 80/92, Loss: 4.5611\n","  Batch 90/92, Loss: 6.0074\n","Epoch 30 finished. Average Loss: 4.9149\n","Saved model after epoch 31 to /content/drive/MyDrive/Projects/Car Detection v2/Colab Env/Model History/singleclass_colab_YOLOv8/weights/epoch_31_last.pt\n","\n","Epoch 31: Training...\n","  Batch 10/92, Loss: 4.0422\n","  Batch 20/92, Loss: 4.0155\n","  Batch 30/92, Loss: 4.9145\n","  Batch 40/92, Loss: 6.8354\n","  Batch 50/92, Loss: 5.3395\n","  Batch 60/92, Loss: 5.1007\n","  Batch 70/92, Loss: 5.3651\n","  Batch 80/92, Loss: 5.8100\n","  Batch 90/92, Loss: 4.7756\n","Epoch 31 finished. Average Loss: 4.8966\n","Saved model after epoch 32 to /content/drive/MyDrive/Projects/Car Detection v2/Colab Env/Model History/singleclass_colab_YOLOv8/weights/epoch_32_last.pt\n","\n","Epoch 32: Training...\n","  Batch 10/92, Loss: 3.8675\n","  Batch 20/92, Loss: 4.5827\n","  Batch 30/92, Loss: 4.5301\n","  Batch 40/92, Loss: 4.0105\n","  Batch 50/92, Loss: 4.4633\n","  Batch 60/92, Loss: 4.7002\n","  Batch 70/92, Loss: 4.6990\n","  Batch 80/92, Loss: 4.4441\n","  Batch 90/92, Loss: 5.6884\n","Epoch 32 finished. Average Loss: 4.8865\n","Saved model after epoch 33 to /content/drive/MyDrive/Projects/Car Detection v2/Colab Env/Model History/singleclass_colab_YOLOv8/weights/epoch_33_last.pt\n","\n","Epoch 33: Training...\n","  Batch 10/92, Loss: 5.0039\n","  Batch 20/92, Loss: 4.8121\n","  Batch 30/92, Loss: 5.1443\n","  Batch 40/92, Loss: 3.7756\n","  Batch 50/92, Loss: 4.6767\n","  Batch 60/92, Loss: 4.9987\n","  Batch 70/92, Loss: 4.5832\n","  Batch 80/92, Loss: 3.8848\n","  Batch 90/92, Loss: 5.3440\n","Epoch 33 finished. Average Loss: 4.8710\n","Saved model after epoch 34 to /content/drive/MyDrive/Projects/Car Detection v2/Colab Env/Model History/singleclass_colab_YOLOv8/weights/epoch_34_last.pt\n","\n","Epoch 34: Training...\n","  Batch 10/92, Loss: 4.7408\n","  Batch 20/92, Loss: 4.7709\n","  Batch 30/92, Loss: 4.6102\n","  Batch 40/92, Loss: 3.8244\n","  Batch 50/92, Loss: 4.2384\n","  Batch 60/92, Loss: 4.2648\n","  Batch 70/92, Loss: 4.8628\n","  Batch 80/92, Loss: 4.1961\n","  Batch 90/92, Loss: 4.5918\n","Epoch 34 finished. Average Loss: 4.8333\n","Saved model after epoch 35 to /content/drive/MyDrive/Projects/Car Detection v2/Colab Env/Model History/singleclass_colab_YOLOv8/weights/epoch_35_last.pt\n","\n","Epoch 35: Training...\n","  Batch 10/92, Loss: 4.8684\n","  Batch 20/92, Loss: 4.4453\n","  Batch 30/92, Loss: 4.9345\n","  Batch 40/92, Loss: 4.9064\n","  Batch 50/92, Loss: 3.1332\n","  Batch 60/92, Loss: 3.5735\n","  Batch 70/92, Loss: 3.6979\n","  Batch 80/92, Loss: 5.4777\n","  Batch 90/92, Loss: 6.6631\n","Epoch 35 finished. Average Loss: 4.7605\n","Saved model after epoch 36 to /content/drive/MyDrive/Projects/Car Detection v2/Colab Env/Model History/singleclass_colab_YOLOv8/weights/epoch_36_last.pt\n","\n","Epoch 36: Training...\n","  Batch 10/92, Loss: 4.7109\n","  Batch 20/92, Loss: 5.6835\n","  Batch 30/92, Loss: 3.6638\n","  Batch 40/92, Loss: 4.5959\n","  Batch 50/92, Loss: 6.4943\n","  Batch 60/92, Loss: 3.5540\n","  Batch 70/92, Loss: 3.7707\n","  Batch 80/92, Loss: 4.5216\n","  Batch 90/92, Loss: 5.7473\n","Epoch 36 finished. Average Loss: 4.7892\n","Saved model after epoch 37 to /content/drive/MyDrive/Projects/Car Detection v2/Colab Env/Model History/singleclass_colab_YOLOv8/weights/epoch_37_last.pt\n","\n","Epoch 37: Training...\n","  Batch 10/92, Loss: 5.5048\n","  Batch 20/92, Loss: 4.7734\n","  Batch 30/92, Loss: 4.7205\n","  Batch 40/92, Loss: 4.2244\n","  Batch 50/92, Loss: 4.6777\n","  Batch 60/92, Loss: 4.0969\n","  Batch 70/92, Loss: 3.6004\n","  Batch 80/92, Loss: 3.9854\n","  Batch 90/92, Loss: 3.7663\n","Epoch 37 finished. Average Loss: 4.7320\n","Saved model after epoch 38 to /content/drive/MyDrive/Projects/Car Detection v2/Colab Env/Model History/singleclass_colab_YOLOv8/weights/epoch_38_last.pt\n","\n","Epoch 38: Training...\n","  Batch 10/92, Loss: 4.3512\n","  Batch 20/92, Loss: 5.9742\n","  Batch 30/92, Loss: 5.0753\n","  Batch 40/92, Loss: 5.0830\n","  Batch 50/92, Loss: 3.5228\n","  Batch 60/92, Loss: 4.3302\n","  Batch 70/92, Loss: 6.1577\n","  Batch 80/92, Loss: 5.8311\n","  Batch 90/92, Loss: 5.0023\n","Epoch 38 finished. Average Loss: 4.7139\n","Saved model after epoch 39 to /content/drive/MyDrive/Projects/Car Detection v2/Colab Env/Model History/singleclass_colab_YOLOv8/weights/epoch_39_last.pt\n","\n","Epoch 39: Training...\n","  Batch 10/92, Loss: 5.7662\n","  Batch 20/92, Loss: 3.8450\n","  Batch 30/92, Loss: 3.6155\n","  Batch 40/92, Loss: 3.4035\n","  Batch 50/92, Loss: 4.7345\n","  Batch 60/92, Loss: 6.7486\n","  Batch 70/92, Loss: 5.2527\n","  Batch 80/92, Loss: 4.4667\n","  Batch 90/92, Loss: 4.5046\n","Epoch 39 finished. Average Loss: 4.7600\n","Saved model after epoch 40 to /content/drive/MyDrive/Projects/Car Detection v2/Colab Env/Model History/singleclass_colab_YOLOv8/weights/epoch_40_last.pt\n","\n","Epoch 40: Training...\n","  Batch 10/92, Loss: 4.7209\n","  Batch 20/92, Loss: 4.3676\n","  Batch 30/92, Loss: 3.2902\n","  Batch 40/92, Loss: 4.6446\n","  Batch 50/92, Loss: 4.0381\n","  Batch 60/92, Loss: 4.9930\n","  Batch 70/92, Loss: 4.4330\n","  Batch 80/92, Loss: 4.4352\n","  Batch 90/92, Loss: 4.1369\n","Epoch 40 finished. Average Loss: 4.7422\n","Saved model after epoch 41 to /content/drive/MyDrive/Projects/Car Detection v2/Colab Env/Model History/singleclass_colab_YOLOv8/weights/epoch_41_last.pt\n","\n","Epoch 41: Training...\n","  Batch 10/92, Loss: 4.7544\n","  Batch 20/92, Loss: 5.5161\n","  Batch 30/92, Loss: 5.7914\n","  Batch 40/92, Loss: 3.9411\n","  Batch 50/92, Loss: 6.5453\n","  Batch 60/92, Loss: 4.0990\n","  Batch 70/92, Loss: 4.4646\n","  Batch 80/92, Loss: 5.8369\n","  Batch 90/92, Loss: 6.5898\n","Epoch 41 finished. Average Loss: 4.6687\n","Saved model after epoch 42 to /content/drive/MyDrive/Projects/Car Detection v2/Colab Env/Model History/singleclass_colab_YOLOv8/weights/epoch_42_last.pt\n","\n","Epoch 42: Training...\n","  Batch 10/92, Loss: 5.4715\n","  Batch 20/92, Loss: 4.1541\n","  Batch 30/92, Loss: 7.4156\n","  Batch 40/92, Loss: 3.9279\n","  Batch 50/92, Loss: 3.9516\n","  Batch 60/92, Loss: 3.9500\n","  Batch 70/92, Loss: 4.7059\n","  Batch 80/92, Loss: 4.1317\n","  Batch 90/92, Loss: 4.0796\n","Epoch 42 finished. Average Loss: 4.7584\n","Saved model after epoch 43 to /content/drive/MyDrive/Projects/Car Detection v2/Colab Env/Model History/singleclass_colab_YOLOv8/weights/epoch_43_last.pt\n","\n","Epoch 43: Training...\n","  Batch 10/92, Loss: 4.8472\n","  Batch 20/92, Loss: 4.6984\n","  Batch 30/92, Loss: 3.9651\n","  Batch 40/92, Loss: 5.4192\n","  Batch 50/92, Loss: 5.6717\n","  Batch 60/92, Loss: 4.2819\n","  Batch 70/92, Loss: 4.8126\n","  Batch 80/92, Loss: 4.3593\n","  Batch 90/92, Loss: 4.9701\n","Epoch 43 finished. Average Loss: 4.6423\n","Saved model after epoch 44 to /content/drive/MyDrive/Projects/Car Detection v2/Colab Env/Model History/singleclass_colab_YOLOv8/weights/epoch_44_last.pt\n","\n","Epoch 44: Training...\n","  Batch 10/92, Loss: 4.5352\n","  Batch 20/92, Loss: 4.7841\n","  Batch 30/92, Loss: 4.8324\n","  Batch 40/92, Loss: 3.5774\n","  Batch 50/92, Loss: 3.5066\n","  Batch 60/92, Loss: 3.9078\n","  Batch 70/92, Loss: 3.5549\n","  Batch 80/92, Loss: 4.4001\n","  Batch 90/92, Loss: 4.7094\n","Epoch 44 finished. Average Loss: 4.6525\n","Saved model after epoch 45 to /content/drive/MyDrive/Projects/Car Detection v2/Colab Env/Model History/singleclass_colab_YOLOv8/weights/epoch_45_last.pt\n","\n","Epoch 45: Training...\n","  Batch 10/92, Loss: 3.4771\n","  Batch 20/92, Loss: 4.5712\n","  Batch 30/92, Loss: 4.7296\n","  Batch 40/92, Loss: 3.9145\n","  Batch 50/92, Loss: 3.9693\n","  Batch 60/92, Loss: 4.5893\n","  Batch 70/92, Loss: 5.2351\n","  Batch 80/92, Loss: 5.5724\n","  Batch 90/92, Loss: 4.8131\n","Epoch 45 finished. Average Loss: 4.5814\n","Saved model after epoch 46 to /content/drive/MyDrive/Projects/Car Detection v2/Colab Env/Model History/singleclass_colab_YOLOv8/weights/epoch_46_last.pt\n","\n","Epoch 46: Training...\n","  Batch 10/92, Loss: 5.4909\n","  Batch 20/92, Loss: 4.4502\n","  Batch 30/92, Loss: 6.1494\n","  Batch 40/92, Loss: 4.8098\n","  Batch 50/92, Loss: 5.5057\n","  Batch 60/92, Loss: 4.2757\n","  Batch 70/92, Loss: 3.4981\n","  Batch 80/92, Loss: 4.5558\n","  Batch 90/92, Loss: 6.4123\n","Epoch 46 finished. Average Loss: 4.5830\n","Saved model after epoch 47 to /content/drive/MyDrive/Projects/Car Detection v2/Colab Env/Model History/singleclass_colab_YOLOv8/weights/epoch_47_last.pt\n","\n","Epoch 47: Training...\n","  Batch 10/92, Loss: 4.6105\n","  Batch 20/92, Loss: 4.6515\n","  Batch 30/92, Loss: 3.9651\n","  Batch 40/92, Loss: 3.4205\n","  Batch 50/92, Loss: 4.1964\n","  Batch 60/92, Loss: 4.0722\n","  Batch 70/92, Loss: 4.0979\n","  Batch 80/92, Loss: 5.1131\n","  Batch 90/92, Loss: 4.2530\n","Epoch 47 finished. Average Loss: 4.5973\n","Saved model after epoch 48 to /content/drive/MyDrive/Projects/Car Detection v2/Colab Env/Model History/singleclass_colab_YOLOv8/weights/epoch_48_last.pt\n","\n","Epoch 48: Training...\n","  Batch 10/92, Loss: 5.0958\n","  Batch 20/92, Loss: 3.9918\n","  Batch 30/92, Loss: 4.4571\n","  Batch 40/92, Loss: 4.5140\n","  Batch 50/92, Loss: 5.4025\n","  Batch 60/92, Loss: 4.6907\n","  Batch 70/92, Loss: 4.8521\n","  Batch 80/92, Loss: 4.0565\n","  Batch 90/92, Loss: 3.8983\n","Epoch 48 finished. Average Loss: 4.5222\n","Saved model after epoch 49 to /content/drive/MyDrive/Projects/Car Detection v2/Colab Env/Model History/singleclass_colab_YOLOv8/weights/epoch_49_last.pt\n","\n","Epoch 49: Training...\n","  Batch 10/92, Loss: 3.7238\n","  Batch 20/92, Loss: 3.7808\n","  Batch 30/92, Loss: 5.5086\n","  Batch 40/92, Loss: 5.2341\n","  Batch 50/92, Loss: 5.3828\n","  Batch 60/92, Loss: 4.0825\n","  Batch 70/92, Loss: 4.1953\n","  Batch 80/92, Loss: 4.1920\n","  Batch 90/92, Loss: 3.5762\n","Epoch 49 finished. Average Loss: 4.5768\n","Saved model after epoch 50 to /content/drive/MyDrive/Projects/Car Detection v2/Colab Env/Model History/singleclass_colab_YOLOv8/weights/epoch_50_last.pt\n","\n","Training completed! Final model saved to /content/drive/MyDrive/Projects/Car Detection v2/Colab Env/Model History/singleclass_colab_YOLOv8/weights/final.pt\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 1000x600 with 1 Axes>"],"image/png":"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\n"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Training history saved to /content/drive/MyDrive/Projects/Car Detection v2/Colab Env/Model History/singleclass_colab_YOLOv8/training_history.json\n"]}]},{"cell_type":"code","source":["\n","# --- Video Inference ---\n","\n","# Define the path to your test video\n","VIDEO_INPUT_PATH = os.path.join(VIDEO_TEST, 'traffic_test.mp4') # REPLACE with your video file name\n","\n","# Define the output path for the labeled video\n","# Ensure the directory exists\n","os.makedirs(VIDEO_TEST_PREDICTION, exist_ok=True)\n","VIDEO_OUTPUT_PATH = os.path.join(VIDEO_TEST_PREDICTION, 'ssd with mobilenetv3 backbone.mp4') # Use .avi format for broader compatibility\n","\n","# Confidence threshold for detections\n","CONF_THRESHOLD = 0.5 # Adjust based on your trained model's performance\n","# IOU threshold for Non-Maximum Suppression (NMS)\n","NMS_THRESHOLD = 0.45 # Common value\n","\n","# Assuming the model is already trained and loaded on 'device' from previous steps\n","# If not, load the best weights:\n","if 'model' not in locals():\n","    print(\"Model not found, creating and loading weights...\")\n","    model = create_ssd_mobilenetv3(NUM_CLASSES) # Create the model structure\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    model.to(device)\n","    best_weights_path = os.path.join(FULL_EXPERIMENT_PATH, 'weights', 'best.pt')\n","    if os.path.exists(best_weights_path):\n","        model.load_state_dict(torch.load(best_weights_path, map_location=device))\n","        print(f\"Loaded best weights from: {best_weights_path}\")\n","    else:\n","        print(f\"WARNING: Best weights not found at {best_weights_path}. Using uninitialized model.\")\n","\n","model.eval() # Set model to evaluation mode\n","\n","# Open the video file\n","cap = cv2.VideoCapture(VIDEO_INPUT_PATH)\n","\n","if not cap.isOpened():\n","    print(f\"Error: Could not open video file {VIDEO_INPUT_PATH}\")\n","else:\n","    # Get video properties\n","    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n","    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n","    fps = int(cap.get(cv2.CAP_PROP_FPS))\n","    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n","\n","    print(f\"Reading video: {VIDEO_INPUT_PATH}\")\n","    print(f\"Frame Dimensions: {frame_width}x{frame_height}\")\n","    print(f\"FPS: {fps}\")\n","    print(f\"Total Frames: {total_frames}\")\n","\n","\n","    # Define the codec and create VideoWriter object\n","    # Use XVID or DIVX codec (might need to install opencv-contrib-python for some codecs)\n","    # fourcc = cv2.VideoWriter_fourcc(*'XVID')\n","    fourcc = cv2.VideoWriter_fourcc(*'MJPG') # MJPG is generally well-supported\n","    out = cv2.VideoWriter(VIDEO_OUTPUT_PATH, fourcc, fps, (frame_width, frame_height))\n","\n","    if not out.isOpened():\n","        print(f\"Error: Could not create video writer for {VIDEO_OUTPUT_PATH}. Make sure the directory exists and you have write permissions.\")\n","        # Close the capture and return if VideoWriter fails\n","        cap.release()\n","    else:\n","        print(f\"Writing labeled video to: {VIDEO_OUTPUT_PATH}\")\n","        frame_count = 0\n","\n","        # Transforms for inference: Resize to 300x300 and convert to tensor\n","        inference_transforms = torchvision.transforms.Compose([\n","            torchvision.transforms.ToPILImage(),\n","            torchvision.transforms.Resize((300, 300)), # SSD input size\n","            torchvision.transforms.ToTensor(),\n","        ])\n","\n","        # Class names (match the order used during training)\n","        CLASS_NAMES = ['__background__', 'car'] # Match this to your actual classes\n","\n","        try:\n","            while True:\n","                ret, frame = cap.read()\n","                if not ret:\n","                    break # End of video\n","\n","                frame_count += 1\n","                # Optional: Print progress\n","                # print(f\"Processing frame {frame_count}/{total_frames}\", end='\\r')\n","\n","                # Convert the frame to RGB (OpenCV reads as BGR)\n","                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n","\n","                # Apply transforms and prepare for model input\n","                # Transforms expect a PIL Image, so convert the NumPy array\n","                frame_tensor = inference_transforms(frame_rgb).unsqueeze(0).to(device) # Add batch dimension and move to device\n","\n","                # Perform inference\n","                with torch.no_grad():\n","                    predictions = model(frame_tensor) # model expects a list of tensors\n","\n","                # Process predictions (the model returns a list of dicts, one for each image in the batch)\n","                # We only have one image in the batch\n","                output = predictions[0]\n","\n","                boxes = output['boxes']\n","                labels = output['labels']\n","                scores = output['scores']\n","\n","                # Filter predictions based on confidence threshold\n","                keep_indices = torch.where(scores > CONF_THRESHOLD)[0]\n","                filtered_boxes = boxes[keep_indices]\n","                filtered_labels = labels[keep_indices]\n","                filtered_scores = scores[keep_indices]\n","\n","                # Apply Non-Maximum Suppression (NMS) - torchvision models already apply NMS by default\n","                # within their internal prediction processing.\n","                # However, if you want to re-apply with a different threshold or visualize before NMS,\n","                # you could do it here. For standard usage, the model's output is usually post-NMS.\n","                # If you were to manually apply NMS:\n","                # keep_after_nms = torchvision.ops.nms(filtered_boxes, filtered_scores, NMS_THRESHOLD)\n","                # final_boxes = filtered_boxes[keep_after_nms]\n","                # final_labels = filtered_labels[keep_after_nms]\n","                # final_scores = filtered_scores[keep_after_nms]\n","                # Using the model's default output which is already NMS'd based on its internal threshold\n","\n","                # Draw bounding boxes and labels on the frame\n","                # The box coordinates are relative to the 300x300 input, scale them back to original frame size\n","                scale_x = frame_width / 300\n","                scale_y = frame_height / 300\n","\n","                # Convert boxes to numpy and scale\n","                scaled_boxes = filtered_boxes.cpu().numpy()\n","                scaled_boxes[:, [0, 2]] *= scale_x # Scale x_min, x_max\n","                scaled_boxes[:, [1, 3]] *= scale_y # Scale y_min, y_max\n","\n","                # Convert labels and scores to numpy/list\n","                final_labels_list = filtered_labels.cpu().numpy().tolist()\n","                final_scores_list = filtered_scores.cpu().numpy().tolist()\n","\n","                # Draw on the original BGR frame for OpenCV VideoWriter\n","                frame_bgr = frame # We draw on the original frame read by cv2\n","\n","                for i in range(len(scaled_boxes)):\n","                    box = scaled_boxes[i]\n","                    label = final_labels_list[i]\n","                    score = final_scores_list[i]\n","\n","                    x1, y1, x2, y2 = map(int, box)\n","\n","                    # Get class name (remember label 0 is background)\n","                    class_name = CLASS_NAMES[label] if label < len(CLASS_NAMES) else f\"Class {label}\"\n","\n","                    # Draw rectangle\n","                    color = (0, 255, 0) # Green color\n","                    cv2.rectangle(frame_bgr, (x1, y1), (x2, y2), color, 2)\n","\n","                    # Put label and score text\n","                    text = f\"{class_name}: {score:.2f}\"\n","                    cv2.putText(frame_bgr, text, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n","\n","                # Write the frame with detections to the output video\n","                out.write(frame_bgr)\n","\n","        finally:\n","            # Release everything when job is finished\n","            cap.release()\n","            out.release()\n","            print(\"\\nVideo inference finished.\")\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bL7tl-rw2_kY","executionInfo":{"status":"ok","timestamp":1748855162393,"user_tz":-420,"elapsed":193215,"user":{"displayName":"Samuel Ady Sanjaya","userId":"15801194796254517624"}},"outputId":"0f2a2275-db27-4330-d6ae-d13a163d2183"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Reading video: /content/drive/MyDrive/Projects/Car Detection v2/Video Test/traffic_test.mp4\n","Frame Dimensions: 1280x720\n","FPS: 30\n","Total Frames: 5920\n","Writing labeled video to: /content/drive/MyDrive/Projects/Car Detection v2/Video Test Prediction Colab Env/ssd with mobilenetv3 backbone.mp4\n","\n","Video inference finished.\n"]}]},{"cell_type":"code","source":["\n","# --- Image Inference ---\n","\n","# Get a list of all image files in the test directory\n","test_image_files = glob.glob(os.path.join(TEST_IMAGES_DIR, '*.jpg')) + \\\n","                   glob.glob(os.path.join(TEST_IMAGES_DIR, '*.jpeg')) + \\\n","                   glob.glob(os.path.join(TEST_IMAGES_DIR, '*.png'))\n","\n","if not test_image_files:\n","    print(f\"Error: No image files found in the test directory: {TEST_IMAGES_DIR}\")\n","else:\n","    # Take 16 random samples\n","    num_samples = min(16, len(test_image_files))\n","    random_sample_paths = random.sample(test_image_files, num_samples)\n","\n","    print(f\"Selected {num_samples} random images from {TEST_IMAGES_DIR} for inference.\")\n","\n","    # Confidence threshold for detections\n","    CONF_THRESHOLD_INFERENCE = 0.5 # Adjust based on your trained model's performance\n","\n","    # Ensure the model is loaded and on the correct device\n","    # This assumes the model variable `model` and device variable `device` are already defined\n","    # and the model is potentially loaded with weights from the training step.\n","    if 'model' not in locals():\n","        print(\"Model not found, creating and loading weights for inference...\")\n","        model = create_ssd_mobilenetv3(NUM_CLASSES) # Create the model structure\n","        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","        model.to(device)\n","        # Load the best weights if they exist\n","        best_weights_path = os.path.join(FULL_EXPERIMENT_PATH, 'weights', 'best.pt')\n","        if os.path.exists(best_weights_path):\n","            model.load_state_dict(torch.load(best_weights_path, map_location=device))\n","            print(f\"Loaded best weights from: {best_weights_path}\")\n","        else:\n","            print(f\"WARNING: Best weights not found at {best_weights_path}. Using uninitialized model for inference.\")\n","    else:\n","        print(\"Using existing model for inference.\")\n","\n","    model.eval() # Set model to evaluation mode\n","\n","    # Transforms for inference: Resize to 300x300 and convert to tensor\n","    inference_transforms = torchvision.transforms.Compose([\n","        torchvision.transforms.ToPILImage(),\n","        torchvision.transforms.Resize((300, 300)), # SSD input size\n","        torchvision.transforms.ToTensor(),\n","    ])\n","\n","    # Class names (match the order used during training)\n","    CLASS_NAMES = ['__background__', 'car'] # Match this to your actual classes\n","\n","    # Create a grid of plots\n","    fig, axs = plt.subplots(4, 4, figsize=(16, 16))\n","    fig.suptitle('Object Detection Inference Results', fontsize=20)\n","\n","    for i, img_path in enumerate(random_sample_paths):\n","        # Read the image using OpenCV\n","        img_bgr = cv2.imread(img_path)\n","        if img_bgr is None:\n","            print(f\"Warning: Could not read image {img_path}. Skipping.\")\n","            continue\n","\n","        # Convert to RGB for model input\n","        img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n","        original_h, original_w, _ = img_rgb.shape\n","\n","        # Apply transforms and prepare for model input\n","        img_tensor = inference_transforms(img_rgb).unsqueeze(0).to(device) # Add batch dimension and move to device\n","\n","        # Perform inference\n","        with torch.no_grad():\n","            predictions = model(img_tensor) # model expects a list of tensors\n","\n","        # Process predictions (the model returns a list of dicts, one for each image in the batch)\n","        # We only have one image in the batch\n","        output = predictions[0]\n","\n","        boxes = output['boxes']\n","        labels = output['labels']\n","        scores = output['scores']\n","\n","        # Filter predictions based on confidence threshold\n","        keep_indices = torch.where(scores > CONF_THRESHOLD_INFERENCE)[0]\n","        filtered_boxes = boxes[keep_indices]\n","        filtered_labels = labels[keep_indices]\n","        filtered_scores = scores[keep_indices]\n","\n","        # Scale the predicted boxes back to the original image size\n","        # The box coordinates are relative to the 300x300 input, scale them back\n","        scale_x = original_w / 300\n","        scale_y = original_h / 300\n","\n","        # Convert boxes to numpy and scale\n","        scaled_boxes = filtered_boxes.cpu().numpy()\n","        scaled_boxes[:, [0, 2]] *= scale_x # Scale x_min, x_max\n","        scaled_boxes[:, [1, 3]] *= scale_y # Scale y_min, y_max\n","\n","        # Convert labels and scores to numpy/list\n","        final_labels_list = filtered_labels.cpu().numpy().tolist()\n","        final_scores_list = filtered_scores.cpu().numpy().tolist()\n","\n","        # Draw bounding boxes and labels on the RGB image for matplotlib display\n","        annotated_img = img_rgb.copy() # Make a copy to draw on\n","\n","        for j in range(len(scaled_boxes)):\n","            box = scaled_boxes[j]\n","            label = final_labels_list[j]\n","            score = final_scores_list[j]\n","\n","            x1, y1, x2, y2 = map(int, box)\n","\n","            # Get class name (remember label 0 is background)\n","            class_name = CLASS_NAMES[label] if label < len(CLASS_NAMES) else f\"Class {label}\"\n","\n","            # Draw rectangle using OpenCV (on the RGB copy)\n","            color = (0, 255, 0) # Green color\n","            cv2.rectangle(annotated_img, (x1, y1), (x2, y2), color, 2)\n","\n","            # Put label and score text using OpenCV\n","            text = f\"{class_name}: {score:.2f}\"\n","            # Define text position (adjust slightly above the box)\n","            text_origin = (x1, y1 - 10 if y1 - 10 > 10 else y1 + 10)\n","            cv2.putText(annotated_img, text, text_origin, cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n","\n","        # Plot the annotated image\n","        ax = axs[i // 4, i % 4]\n","        ax.imshow(annotated_img)\n","        ax.set_title(os.path.basename(img_path), fontsize=8)\n","        ax.axis('off')\n","\n","    plt.tight_layout(rect=[0, 0, 1, 0.96]) # Adjust layout to make space for suptitle\n","    plt.show()\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1H-fiZ3DLWVkRhpeMx1EGSnRALjHx1U0Y"},"id":"SIeiRcCN8ugl","executionInfo":{"status":"ok","timestamp":1748858660866,"user_tz":-420,"elapsed":3393,"user":{"displayName":"Samuel Ady Sanjaya","userId":"15801194796254517624"}},"outputId":"89518081-91ce-4aea-85ab-5fa16345c5fb"},"execution_count":39,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","source":["# prompt: I have a video in mp4 file for testing. create code for video inference or labeled video that already predicted from the model ssd with mobilenetv3 backbone\n","\n","# --- Video Inference ---\n","\n","# Define the path to your test video\n","VIDEO_INPUT_PATH = os.path.join(VIDEO_TEST, 'traffic_test.mp4') # REPLACE with your video file name\n","\n","# Define the output path for the labeled video\n","# Ensure the directory exists\n","os.makedirs(VIDEO_TEST_PREDICTION, exist_ok=True)\n","# Changed to .avi - often more compatible with different codecs\n","VIDEO_OUTPUT_PATH = os.path.join(VIDEO_TEST_PREDICTION, 'ssd with mobilenetv3 backbone (1).mp4')\n","\n","# Confidence threshold for detections\n","CONF_THRESHOLD = 0.5 # Adjust based on your trained model's performance\n","# IOU threshold for Non-Maximum Suppression (NMS)\n","# Note: torchvision models typically apply NMS internally.\n","# This threshold might not be directly used here unless you re-implement NMS.\n","NMS_THRESHOLD = 0.45 # Common value\n","\n","# Assuming the model is already trained and loaded on 'device' from previous steps\n","# If not, load the best weights:\n","if 'model' not in locals():\n","    print(\"Model not found, creating and loading weights...\")\n","    # Ensure NUM_CLASSES is defined (it should be from previous cells)\n","    if 'NUM_CLASSES' not in locals():\n","        print(\"ERROR: NUM_CLASSES not defined. Assuming 2 (car + background).\")\n","        NUM_CLASSES = 2\n","\n","    model = create_ssd_mobilenetv3(NUM_CLASSES) # Create the model structure\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    model.to(device)\n","    best_weights_path = os.path.join(FULL_EXPERIMENT_PATH, 'weights', 'best.pt')\n","    if os.path.exists(best_weights_path):\n","        try:\n","            # Use map_location to load weights correctly regardless of where it was saved\n","            model.load_state_dict(torch.load(best_weights_path, map_location=device))\n","            print(f\"Loaded best weights from: {best_weights_path}\")\n","        except Exception as e:\n","             print(f\"Error loading model weights from {best_weights_path}: {e}\")\n","             print(\"Using uninitialized model.\")\n","    else:\n","        print(f\"WARNING: Best weights not found at {best_weights_path}. Using uninitialized model.\")\n","\n","model.eval() # Set model to evaluation mode\n","\n","# Open the video file\n","cap = cv2.VideoCapture(VIDEO_INPUT_PATH)\n","\n","if not cap.isOpened():\n","    print(f\"Error: Could not open video file {VIDEO_INPUT_PATH}\")\n","else:\n","    # Get video properties\n","    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n","    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n","    fps = int(cap.get(cv2.CAP_PROP_FPS))\n","    # In some cases, total_frames might be -1. Handle this if needed.\n","    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n","\n","    print(f\"Reading video: {VIDEO_INPUT_PATH}\")\n","    print(f\"Frame Dimensions: {frame_width}x{frame_height}\")\n","    print(f\"FPS: {fps}\")\n","    print(f\"Total Frames: {total_frames if total_frames > 0 else 'Unknown'}\")\n","\n","\n","    # Define the codec and create VideoWriter object\n","    # Use a common codec like MJPG or XVID.\n","    # 'mp4v' or 'avc1' might also work depending on your OpenCV build and installed codecs.\n","    # fourcc = cv2.VideoWriter_fourcc(*'XVID')\n","    # fourcc = cv2.VideoWriter_fourcc(*'DIVX')\n","    fourcc = cv2.VideoWriter_fourcc(*'MJPG') # MJPG is generally well-supported and produces .avi\n","\n","    # The output frame size MUST match the frame size read from the input video\n","    out = cv2.VideoWriter(VIDEO_OUTPUT_PATH, fourcc, fps, (frame_width, frame_height))\n","\n","    if not out.isOpened():\n","        print(f\"Error: Could not create video writer for {VIDEO_OUTPUT_PATH}.\")\n","        print(\"Common issues: Invalid codec (fourcc), insufficient permissions, or incorrect path.\")\n","        print(\"Attempting with a different codec ('mp4v')...\")\n","        try:\n","             fourcc_mp4v = cv2.VideoWriter_fourcc(*'mp4v')\n","             out = cv2.VideoWriter(VIDEO_OUTPUT_PATH.replace('.avi', '.mp4'), fourcc_mp4v, fps, (frame_width, frame_height))\n","             if out.isOpened():\n","                 VIDEO_OUTPUT_PATH = VIDEO_OUTPUT_PATH.replace('.avi', '.mp4')\n","                 print(f\"Successfully created video writer with 'mp4v' codec: {VIDEO_OUTPUT_PATH}\")\n","             else:\n","                 print(\"Second attempt with 'mp4v' also failed. Please check your OpenCV installation and codecs.\")\n","                 # Close the capture and return if VideoWriter fails\n","                 cap.release()\n","                 # Ensure 'out' is None or handle appropriately if creation failed\n","                 out = None # Explicitly set out to None if it failed\n","        except Exception as e:\n","            print(f\"Error during second attempt with 'mp4v': {e}\")\n","            cap.release()\n","            out = None\n","    else:\n","        print(f\"Writing labeled video to: {VIDEO_OUTPUT_PATH}\")\n","        frame_count = 0\n","\n","        # Transforms for inference: Resize to 300x300 and convert to tensor\n","        # These transforms are applied BEFORE feeding to the model.\n","        inference_transforms = torchvision.transforms.Compose([\n","            torchvision.transforms.ToPILImage(),\n","            torchvision.transforms.Resize((300, 300)), # SSD input size\n","            torchvision.transforms.ToTensor(),\n","        ])\n","\n","        # Class names (match the order used during training)\n","        # Ensure this matches your dataset's class mapping.\n","        CLASS_NAMES = ['__background__', 'car'] # Match this to your actual classes\n","\n","        # Check if video writer was successfully created before entering the loop\n","        if out is not None and out.isOpened():\n","            try:\n","                while True:\n","                    ret, frame = cap.read() # frame is a NumPy array, BGR format\n","                    if not ret:\n","                        break # End of video\n","\n","                    frame_count += 1\n","                    # Optional: Print progress\n","                    if frame_count % 100 == 0: # Print every 100 frames\n","                         print(f\"Processing frame {frame_count}{'/' + str(total_frames) if total_frames > 0 else ''}\", end='\\r')\n","\n","\n","                    # *** IMPORTANT: Create a copy of the frame for drawing ***\n","                    # This prevents potential issues if the original frame is modified unexpectedly\n","                    frame_to_draw = frame.copy()\n","\n","                    # Convert the frame to RGB (needed for the transforms)\n","                    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n","\n","                    # Apply transforms and prepare for model input\n","                    frame_tensor = inference_transforms(frame_rgb).unsqueeze(0).to(device) # Add batch dimension and move to device\n","\n","                    # Perform inference\n","                    with torch.no_grad():\n","                        # model expects a list of tensors, returns a list of dicts\n","                        predictions = model(frame_tensor)\n","\n","                    # Process predictions (the model returns a list of dicts, one for each image in the batch)\n","                    # We only have one image in the batch\n","                    output = predictions[0]\n","\n","                    boxes = output['boxes']\n","                    labels = output['labels']\n","                    scores = output['scores']\n","\n","                    # Filter predictions based on confidence threshold\n","                    keep_indices = torch.where(scores > CONF_THRESHOLD)[0]\n","                    filtered_boxes = boxes[keep_indices]\n","                    filtered_labels = labels[keep_indices]\n","                    filtered_scores = scores[keep_indices]\n","\n","                    # --- Drawing Bounding Boxes and Labels ---\n","                    # The box coordinates are relative to the 300x300 input.\n","                    # Scale them back to the original frame size before drawing.\n","                    scale_x = frame_width / 300.0 # Use float division\n","                    scale_y = frame_height / 300.0\n","\n","                    # Convert boxes to numpy and scale\n","                    # Ensure moving to CPU before converting to numpy if on GPU\n","                    scaled_boxes = filtered_boxes.cpu().numpy()\n","                    scaled_boxes[:, [0, 2]] *= scale_x # Scale x_min, x_max\n","                    scaled_boxes[:, [1, 3]] *= scale_y # Scale y_min, y_max\n","\n","                    # Convert labels and scores to numpy/list\n","                    final_labels_list = filtered_labels.cpu().numpy().tolist()\n","                    final_scores_list = filtered_scores.cpu().numpy().tolist()\n","\n","                    # Draw on the frame_to_draw (which is BGR format, suitable for cv2)\n","                    for i in range(len(scaled_boxes)):\n","                        box = scaled_boxes[i]\n","                        label = final_labels_list[i]\n","                        score = final_scores_list[i]\n","\n","                        x1, y1, x2, y2 = map(int, box) # Convert coordinates to integers\n","\n","                        # Get class name (remember label 0 is background in SSD)\n","                        # Ensure the label index is valid for CLASS_NAMES\n","                        class_name = CLASS_NAMES[label] if label >= 0 and label < len(CLASS_NAMES) else f\"Unknown Class {label}\"\n","\n","                        # Draw rectangle\n","                        color = (0, 255, 0) # Green color (BGR format)\n","                        thickness = 2\n","                        cv2.rectangle(frame_to_draw, (x1, y1), (x2, y2), color, thickness)\n","\n","                        # Put label and score text\n","                        text = f\"{class_name}: {score:.2f}\"\n","                        font = cv2.FONT_HERSHEY_SIMPLEX\n","                        font_scale = 0.6\n","                        font_thickness = 2\n","                        # Position the text slightly above the top-left corner of the box\n","                        text_origin = (x1, y1 - 10 if y1 - 10 > 10 else y1 + 10) # Avoid going out of bounds\n","                        cv2.putText(frame_to_draw, text, text_origin, font, font_scale, color, font_thickness)\n","\n","                    # Write the frame with detections to the output video\n","                    # *** Write the modified frame_to_draw ***\n","                    out.write(frame_to_draw)\n","\n","            except Exception as e:\n","                 print(f\"\\nAn error occurred during video processing: {e}\")\n","                 # Important: Release resources even if an error occurs\n","            finally:\n","                # Release everything when job is finished\n","                cap.release()\n","                # Only release 'out' if it was successfully created\n","                if out is not None and out.isOpened():\n","                    out.release()\n","                print(\"\\nVideo inference finished.\")\n","        else:\n","            print(\"Video writer was not successfully created. Cannot proceed with processing.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EDdIiY197DY9","executionInfo":{"status":"ok","timestamp":1748855938720,"user_tz":-420,"elapsed":193334,"user":{"displayName":"Samuel Ady Sanjaya","userId":"15801194796254517624"}},"outputId":"ae6536aa-da21-4510-d9b7-83efd7a95158"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Reading video: /content/drive/MyDrive/Projects/Car Detection v2/Video Test/traffic_test.mp4\n","Frame Dimensions: 1280x720\n","FPS: 30\n","Total Frames: 5920\n","Writing labeled video to: /content/drive/MyDrive/Projects/Car Detection v2/Video Test Prediction Colab Env/ssd with mobilenetv3 backbone (1).mp4\n","\n","Video inference finished.\n"]}]}]}