{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"T4","authorship_tag":"ABX9TyNYfQGFU6oymSIbTz6YDZ4k"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["## 1. Install Dependencies"],"metadata":{"id":"twZmAIyXcxuZ"}},{"cell_type":"code","source":["!pip install torch torchvision torchmetrics"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TjWu7Ippd-P5","executionInfo":{"status":"ok","timestamp":1749051953904,"user_tz":-420,"elapsed":67270,"user":{"displayName":"Samuel Ady Sanjaya","userId":"15801194796254517624"}},"outputId":"2f8c8fe1-9bfc-461b-bad4-4d68d6090bb9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n","Collecting torchmetrics\n","  Downloading torchmetrics-1.7.2-py3-none-any.whl.metadata (21 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n","Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n","  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n","  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n","  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n","  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n","  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n","  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n","  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n","  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n","  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n","Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (24.2)\n","Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n","  Downloading lightning_utilities-0.14.3-py3-none-any.whl.metadata (5.6 kB)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.2.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n","Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m100.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m85.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m43.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m106.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading torchmetrics-1.7.2-py3-none-any.whl (962 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m962.5/962.5 kB\u001b[0m \u001b[31m58.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading lightning_utilities-0.14.3-py3-none-any.whl (28 kB)\n","Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, lightning-utilities, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torchmetrics\n","  Attempting uninstall: nvidia-nvjitlink-cu12\n","    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n","    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n","      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n","  Attempting uninstall: nvidia-curand-cu12\n","    Found existing installation: nvidia-curand-cu12 10.3.6.82\n","    Uninstalling nvidia-curand-cu12-10.3.6.82:\n","      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n","  Attempting uninstall: nvidia-cufft-cu12\n","    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n","    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n","      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n","  Attempting uninstall: nvidia-cuda-runtime-cu12\n","    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n","    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n","    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n","    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-cupti-cu12\n","    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n","    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n","  Attempting uninstall: nvidia-cublas-cu12\n","    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n","    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n","      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n","  Attempting uninstall: nvidia-cusparse-cu12\n","    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n","    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n","      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n","  Attempting uninstall: nvidia-cudnn-cu12\n","    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n","    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n","      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n","  Attempting uninstall: nvidia-cusolver-cu12\n","    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n","    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n","      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n","Successfully installed lightning-utilities-0.14.3 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 torchmetrics-1.7.2\n"]}]},{"cell_type":"markdown","source":["## 2. Imports & Helper Functions"],"metadata":{"id":"E7-hpG6lczbe"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WhB84osex5cH","executionInfo":{"status":"ok","timestamp":1749052005188,"user_tz":-420,"elapsed":51282,"user":{"displayName":"Samuel Ady Sanjaya","userId":"15801194796254517624"}},"outputId":"b9399573-5b16-47cc-eb17-b23a2bbb00f1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["# === System & I/O ===\n","import os\n","import random\n","from pathlib import Path\n","\n","# === Deep Learning ===\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms as T\n","from torchvision.models.detection import fasterrcnn_resnet50_fpn\n","from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n","from torchvision.models.detection.rpn import AnchorGenerator\n","\n","# === Visualization ===\n","import cv2\n","import matplotlib.pyplot as plt\n","import matplotlib.image as mpimg"],"metadata":{"id":"Z2A0_cF0eAYV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def collate_fn(batch):\n","    return tuple(zip(*batch))\n","\n","def get_transform(train):\n","    transforms = [T.ToTensor()]\n","    if train:\n","        # randomly flip to augment\n","        transforms.append(T.RandomHorizontalFlip(0.5))\n","    return T.Compose(transforms)\n"],"metadata":{"id":"QXz2q5c5eFNP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 3. Dataset Class (YOLO→Torch Detection Format)"],"metadata":{"id":"lfBiGf9hczYx"}},{"cell_type":"code","source":["class YoloDataset(Dataset):\n","    def __init__(self, img_dir, label_dir, transforms=None):\n","        self.img_dir    = Path(img_dir)\n","        self.label_dir  = Path(label_dir)\n","        self.transforms = transforms\n","        self.images     = sorted(self.img_dir.glob('*.jpg'))\n","\n","    def __len__(self):\n","        return len(self.images)\n","\n","    def __getitem__(self, idx):\n","        # --- Load image ---\n","        img_path = self.images[idx]\n","        img      = cv2.imread(str(img_path))\n","        img      = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","\n","        # --- Load & parse labels ---\n","        label_path = self.label_dir / f\"{img_path.stem}.txt\"\n","        boxes, labels = [], []\n","        h, w = img.shape[:2]\n","        # Check if the label file exists and is not empty\n","        if label_path.exists() and os.path.getsize(label_path) > 0:\n","            for row in open(label_path):\n","                cls, x_c, y_c, bw, bh = map(float, row.split())\n","                x1 = (x_c - bw/2) * w\n","                y1 = (y_c - bh/2) * h\n","                x2 = (x_c + bw/2) * w\n","                y2 = (y_c + bh/2) * h\n","                boxes.append([x1, y1, x2, y2])\n","                labels.append(int(cls) + 1)\n","        if len(boxes) > 0:\n","             boxes_tensor = torch.tensor(boxes, dtype=torch.float32)\n","        else:\n","             boxes_tensor = torch.empty((0, 4), dtype=torch.float32)\n","\n","        labels_tensor = torch.tensor(labels, dtype=torch.int64)\n","\n","        if len(boxes) > 0:\n","             area = (boxes_tensor[:, 3] - boxes_tensor[:, 1]) * (boxes_tensor[:, 2] - boxes_tensor[:, 0])\n","        else:\n","             # If no boxes, area should be an empty tensor with the same dtype\n","             area = torch.tensor([], dtype=torch.float32)\n","\n","\n","        # --- Build target dict ---\n","        target = {\n","            'boxes': boxes_tensor,\n","            'labels': labels_tensor,\n","            'image_id': torch.tensor([idx]),\n","            'area': area,\n","            'iscrowd': torch.zeros((len(boxes),), dtype=torch.int64)\n","        }\n","\n","        if self.transforms:\n","            img = self.transforms(img)\n","\n","        return img, target"],"metadata":{"id":"mAvHdHY2u9gl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 4. Prepare DataLoaders"],"metadata":{"id":"bBp09G34czV4"}},{"cell_type":"code","source":["base = '/content/drive/MyDrive/Projects/Car Detection v2/Single Class Data'\n","\n","train_ds = YoloDataset(f\"{base}/train/images\",\n","                       f\"{base}/train/labels\",\n","                       transforms=get_transform(train=True))\n","val_ds   = YoloDataset(f\"{base}/valid/images\",\n","                       f\"{base}/valid/labels\",\n","                       transforms=get_transform(train=False))\n","\n","train_loader = DataLoader(train_ds, batch_size=16, shuffle=True,\n","                          collate_fn=collate_fn, num_workers=4)\n","val_loader   = DataLoader(val_ds,   batch_size=16, shuffle=False,\n","                          collate_fn=collate_fn, num_workers=4)\n","\n","device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"],"metadata":{"id":"fvP7-wyXeILb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 5. Build & Customize Faster R-CNN"],"metadata":{"id":"d7OBhkaBczSs"}},{"cell_type":"code","source":["import torch\n","from torchvision.models.detection import fasterrcnn_resnet50_fpn\n","from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n","from torch.utils.data import DataLoader"],"metadata":{"id":"piQ-m5P_k-1b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = fasterrcnn_resnet50_fpn(pretrained=True)\n","in_features = model.roi_heads.box_predictor.cls_score.in_features\n","\n","num_classes = 2\n","model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n","model.to(device)\n","\n","params = [p for p in model.parameters() if p.requires_grad]\n","optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n","lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GWM-pvueq0Zk","executionInfo":{"status":"ok","timestamp":1749052023224,"user_tz":-420,"elapsed":2023,"user":{"displayName":"Samuel Ady Sanjaya","userId":"15801194796254517624"}},"outputId":"2a798a2c-09ef-472a-c4b7-8f3e19bd6345"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\" to /root/.cache/torch/hub/checkpoints/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\n","100%|██████████| 160M/160M [00:00<00:00, 185MB/s]\n"]}]},{"cell_type":"code","source":["import torch\n","from torchmetrics.detection import MeanAveragePrecision\n","\n","num_epochs = 10\n","\n","map_metric = MeanAveragePrecision(iou_type=\"bbox\")\n","map_metric.to(device)\n","\n","for epoch in range(num_epochs):\n","    # --- Train ---\n","    model.train()\n","    epoch_loss_train = {}\n","    for images, targets in train_loader:\n","        images = [img.to(device) for img in images]\n","        targets_train = [{k: v.to(device) for k, v in t.items()} for t in targets]\n","\n","        # In train mode, model returns a dictionary of losses\n","        loss_dict = model(images, targets_train)\n","        loss = sum(l for l in loss_dict.values())\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        # Accumulate training losses\n","        for k, v in loss_dict.items():\n","            epoch_loss_train[k] = epoch_loss_train.get(k, 0.0) + v.item()\n","\n","    # Average training losses over batches\n","    for k in epoch_loss_train:\n","        epoch_loss_train[k] /= len(train_loader)\n","\n","    print(f\"Epoch {epoch+1}/{num_epochs} — Train losses: {epoch_loss_train}\")\n","\n","    model.eval()\n","    with torch.no_grad():\n","        for images, targets in val_loader:\n","            images_map = [img.to(device) for img in images]\n","            targets_map = [{k: v.to(device) for k, v in t.items()} for t in targets]\n","            # In eval mode, model returns predictions (list of dicts)\n","            predictions = model(images_map)\n","            map_metric.update(predictions, targets_map)\n","\n","    # Compute the mAP scores\n","    try:\n","        eval_results = map_metric.compute()\n","        map50 = eval_results.get('map_50', torch.tensor(0.0)).item()\n","        map75 = eval_results.get('map_75', torch.tensor(0.0)).item()\n","        map_avg = eval_results.get('map', eval_results.get('maps', [torch.tensor(0.0)])[-1]).item()\n","\n","\n","        print(f\"Epoch {epoch+1}/{num_epochs} — mAP@50: {map50:.4f}, mAP@75: {map75:.4f}, mAP: {map_avg:.4f}\\n\")\n","    except Exception as e:\n","        print(f\"Epoch {epoch+1}/{num_epochs} — Could not compute mAP: {e}\\n\")\n","\n","    map_metric.reset()\n","    lr_scheduler.step()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8ihLf9J8x_Cq","executionInfo":{"status":"ok","timestamp":1749054718977,"user_tz":-420,"elapsed":2695752,"user":{"displayName":"Samuel Ady Sanjaya","userId":"15801194796254517624"}},"outputId":"bd7127ca-4394-4a23-8841-528856e52011"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10 — Train losses: {'loss_classifier': 0.24785451153698174, 'loss_box_reg': 0.3054098414014215, 'loss_objectness': 0.12355333437090335, 'loss_rpn_box_reg': 0.031490401334493705}\n","Epoch 1/10 — mAP@50: 0.8043, mAP@75: 0.3520, mAP: 0.4019\n","\n","Epoch 2/10 — Train losses: {'loss_classifier': 0.2538409409639628, 'loss_box_reg': 0.32554298817463545, 'loss_objectness': 0.07842654269188643, 'loss_rpn_box_reg': 0.02558721121117149}\n","Epoch 2/10 — mAP@50: 0.8294, mAP@75: 0.4035, mAP: 0.4306\n","\n","Epoch 3/10 — Train losses: {'loss_classifier': 0.2605525465763133, 'loss_box_reg': 0.3366597051853719, 'loss_objectness': 0.06759690645191332, 'loss_rpn_box_reg': 0.02340739545331377}\n","Epoch 3/10 — mAP@50: 0.8143, mAP@75: 0.3912, mAP: 0.4246\n","\n","Epoch 4/10 — Train losses: {'loss_classifier': 0.26758678970129596, 'loss_box_reg': 0.3448081973778165, 'loss_objectness': 0.0600621964296569, 'loss_rpn_box_reg': 0.022594123744689252}\n","Epoch 4/10 — mAP@50: 0.8185, mAP@75: 0.3929, mAP: 0.4277\n","\n","Epoch 5/10 — Train losses: {'loss_classifier': 0.2612685577377029, 'loss_box_reg': 0.34012841838209523, 'loss_objectness': 0.057708567291822124, 'loss_rpn_box_reg': 0.021906684699427824}\n","Epoch 5/10 — mAP@50: 0.8286, mAP@75: 0.4111, mAP: 0.4384\n","\n","Epoch 6/10 — Train losses: {'loss_classifier': 0.2623158733805884, 'loss_box_reg': 0.3432873963661816, 'loss_objectness': 0.05544616605924523, 'loss_rpn_box_reg': 0.021623681385434516}\n","Epoch 6/10 — mAP@50: 0.8264, mAP@75: 0.4107, mAP: 0.4384\n","\n","Epoch 7/10 — Train losses: {'loss_classifier': 0.26825353794771695, 'loss_box_reg': 0.3493098352590333, 'loss_objectness': 0.056477261828663555, 'loss_rpn_box_reg': 0.022235822620923103}\n","Epoch 7/10 — mAP@50: 0.8219, mAP@75: 0.4078, mAP: 0.4379\n","\n","Epoch 8/10 — Train losses: {'loss_classifier': 0.2617189519755218, 'loss_box_reg': 0.34091208551241003, 'loss_objectness': 0.05450254796153825, 'loss_rpn_box_reg': 0.021294386753731447}\n","Epoch 8/10 — mAP@50: 0.8234, mAP@75: 0.4119, mAP: 0.4382\n","\n","Epoch 9/10 — Train losses: {'loss_classifier': 0.26511399091585824, 'loss_box_reg': 0.3442841016727945, 'loss_objectness': 0.05512768392572585, 'loss_rpn_box_reg': 0.02141942679072204}\n","Epoch 9/10 — mAP@50: 0.8221, mAP@75: 0.4093, mAP: 0.4381\n","\n","Epoch 10/10 — Train losses: {'loss_classifier': 0.2677134464940299, 'loss_box_reg': 0.3464266320933466, 'loss_objectness': 0.05630593921017388, 'loss_rpn_box_reg': 0.021669171284884214}\n","Epoch 10/10 — mAP@50: 0.8220, mAP@75: 0.4099, mAP: 0.4385\n","\n"]}]},{"cell_type":"markdown","source":["## 9. Still Image Inference"],"metadata":{"id":"vbf0rMQPczBO"}},{"cell_type":"code","source":["import os\n","import random\n","import cv2\n","import torch\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","def infer_image(\n","    model,\n","    img_path: str,\n","    device,\n","    category_names: list,\n","    score_thresh: float = 0.5,\n","    alpha: float = 0.4\n","):\n","    # load & preprocess\n","    img_bgr = cv2.imread(img_path)\n","    img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n","    tensor  = torch.from_numpy(img_rgb/255.).permute(2,0,1).float().to(device)\n","\n","    # inference\n","    model.eval()\n","    with torch.no_grad():\n","        outputs = model([tensor])[0]\n","\n","    boxes  = outputs['boxes'].cpu().numpy()\n","    scores = outputs['scores'].cpu().numpy()\n","    labels = outputs['labels'].cpu().numpy()\n","\n","    # overlay for semi-transparent fill\n","    overlay = img_rgb.copy()\n","    for box, score in zip(boxes, scores):\n","        if score < score_thresh: continue\n","        x1,y1,x2,y2 = box.astype(int)\n","        cv2.rectangle(overlay, (x1,y1), (x2,y2), (0,0,0), -1)\n","\n","    annotated = cv2.addWeighted(overlay, alpha, img_rgb, 1-alpha, 0)\n","\n","    # draw borders + labels\n","    for box, score, label in zip(boxes, scores, labels):\n","        if score < score_thresh: continue\n","        x1,y1,x2,y2 = box.astype(int)\n","        name = category_names[label] if label < len(category_names) else str(label)\n","        text = f\"{name}: {score:.2f}\"\n","\n","        # border\n","        cv2.rectangle(annotated, (x1,y1), (x2,y2), (0,0,255), 2)\n","\n","        # text background\n","        (tw, th), baseline = cv2.getTextSize(text,\n","                                             cv2.FONT_HERSHEY_SIMPLEX,\n","                                             0.5, 1)\n","        cv2.rectangle(annotated,\n","                      (x1, y1-th-baseline),\n","                      (x1+tw, y1),\n","                      (0,0,255), -1)\n","\n","        # text\n","        cv2.putText(annotated, text,\n","                    (x1, y1-baseline),\n","                    cv2.FONT_HERSHEY_SIMPLEX,\n","                    0.5, (255,255,255), 1,\n","                    lineType=cv2.LINE_AA)\n","\n","    return annotated\n","\n","# --- 2. Sample 16 random test images and plot ---\n","custom_image_dir = '/content/drive/MyDrive/Projects/Car Detection v2/Single Class Data/test/images'\n","image_files = [f for f in os.listdir(custom_image_dir) if f.lower().endswith(('.jpg','.png','.jpeg'))]\n","\n","selected_images = random.sample(image_files, 16)\n","\n","fig, axes = plt.subplots(nrows=4, ncols=4, figsize=(15, 15))\n","for i, img_file in enumerate(selected_images):\n","    row, col = divmod(i, 4)\n","    img_path = os.path.join(custom_image_dir, img_file)\n","    detected = infer_image(\n","        model=model,\n","        img_path=img_path,\n","        device=device,\n","        category_names=['__background__','car'],  # adjust as needed\n","        score_thresh=0.5,\n","        alpha=0.4\n","    )\n","    axes[row, col].imshow(detected)\n","    axes[row, col].axis('off')\n","\n","plt.subplots_adjust(wspace=0.05, hspace=0.05)\n","plt.show()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1yb-WrYpp1EZBjle6ux0HhbiMlt3lCqXz"},"id":"-mxk3LiOdVDO","executionInfo":{"status":"ok","timestamp":1749054726144,"user_tz":-420,"elapsed":7159,"user":{"displayName":"Samuel Ady Sanjaya","userId":"15801194796254517624"}},"outputId":"d462ffec-6e27-469d-a3fd-75d8405d66ab"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"markdown","source":["## 10. Video Inference"],"metadata":{"id":"ZzeHB2Ukcy-D"}},{"cell_type":"code","source":["# --- 2. VIDEO INFERENCE & EXPORT ---\n","def process_video(model, in_path, out_path, device, score_thresh=0.5, alpha=0.4, print_every=30):\n","    \"\"\"\n","    Reads in_path, runs detection frame by frame, draws masks+boxes,\n","    and writes the result to out_path.\n","    \"\"\"\n","    cap   = cv2.VideoCapture(in_path)\n","    fps   = int(cap.get(cv2.CAP_PROP_FPS))\n","    w     = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n","    h     = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n","    fourcc= cv2.VideoWriter_fourcc(*'mp4v')\n","    writer= cv2.VideoWriter(out_path, fourcc, fps, (w,h))\n","\n","    frame_idx = 0\n","    while True:\n","        ret, frame = cap.read()\n","        if not ret:\n","            break\n","\n","        # convert BGR→RGB & to tensor\n","        rgb    = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n","        tensor = torch.from_numpy(rgb/255.).permute(2,0,1).float().to(device)\n","\n","        # inference\n","        with torch.no_grad():\n","            out = model([tensor])[0]\n","\n","        boxes  = out['boxes'].cpu().numpy()\n","        scores = out['scores'].cpu().numpy()\n","\n","        overlay = rgb.copy()\n","        for box, score in zip(boxes, scores):\n","            if score < score_thresh:\n","                continue\n","            x1,y1,x2,y2 = box.astype(int)\n","            cv2.rectangle(overlay, (x1,y1), (x2,y2), (0,0,0), -1)\n","            cv2.rectangle(rgb,     (x1,y1), (x2,y2), (0,0,255), 2)\n","\n","        # blend & convert back to BGR for writer\n","        blended = cv2.addWeighted(overlay, alpha, rgb, 1-alpha, 0)\n","        out_bgr = cv2.cvtColor(blended, cv2.COLOR_RGB2BGR)\n","        writer.write(out_bgr)\n","\n","        frame_idx += 1\n","        if frame_idx % print_every == 0:\n","            print(f\"[Frame {frame_idx}] processed.\")\n","\n","    cap.release()\n","    writer.release()\n","    print(\"Video saved to:\", out_path)\n"],"metadata":{"id":"xNCwiDvgd4VA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import cv2\n","import torch\n","\n","def process_video(model,\n","                  in_path,\n","                  out_path,\n","                  device,\n","                  category_names,\n","                  score_thresh=0.5,\n","                  alpha=0.4,\n","                  print_every=30):\n","    \"\"\"\n","    Reads in_path, runs detection frame by frame, draws masks+boxes+labels,\n","    and writes the result to out_path.\n","\n","    Args:\n","        model: a detection model returning dicts with 'boxes', 'labels', 'scores'\n","        in_path: input video file\n","        out_path: output video file\n","        device: torch.device\n","        category_names: list of class names, e.g. [\"__background__\", \"car\"]\n","        score_thresh: minimum confidence\n","        alpha: blending factor for overlay\n","        print_every: how often to log progress\n","    \"\"\"\n","    cap    = cv2.VideoCapture(in_path)\n","    fps    = int(cap.get(cv2.CAP_PROP_FPS))\n","    w      = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n","    h      = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n","    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n","    writer = cv2.VideoWriter(out_path, fourcc, fps, (w, h))\n","\n","    frame_idx = 0\n","    while True:\n","        ret, frame = cap.read()\n","        if not ret:\n","            break\n","\n","        # BGR→RGB & to tensor\n","        rgb    = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n","        tensor = torch.from_numpy(rgb / 255.0).permute(2, 0, 1).float().to(device)\n","\n","        # inference\n","        with torch.no_grad():\n","            out = model([tensor])[0]\n","\n","        boxes  = out['boxes'].cpu().numpy()\n","        labels = out['labels'].cpu().numpy()\n","        scores = out['scores'].cpu().numpy()\n","\n","        overlay = rgb.copy()\n","        for box, label, score in zip(boxes, labels, scores):\n","            if score < score_thresh:\n","                continue\n","\n","            x1, y1, x2, y2 = box.astype(int)\n","            class_name    = category_names[label]\n","            text          = f\"{class_name}: {score:.2f}\"\n","\n","            # draw a semi-transparent fill under the box\n","            # cv2.rectangle(overlay, (x1, y1), (x2, y2), (0, 0, 0), -1)\n","            # draw the box border\n","            cv2.rectangle(rgb, (x1, y1), (x2, y2), (0, 0, 255), 2)\n","            # draw the text on the rgb frame (above the box)\n","            cv2.putText(rgb,\n","                        text,\n","                        (x1, max(y1 - 10, 0)),\n","                        cv2.FONT_HERSHEY_SIMPLEX,\n","                        0.5,\n","                        (255, 255, 255),\n","                        2,\n","                        lineType=cv2.LINE_AA)\n","\n","        # blend & convert back to BGR for writing\n","        blended = cv2.addWeighted(overlay, alpha, rgb, 1 - alpha, 0)\n","        out_bgr = cv2.cvtColor(blended, cv2.COLOR_RGB2BGR)\n","        writer.write(out_bgr)\n","\n","        frame_idx += 1\n","        if frame_idx % print_every == 0:\n","            print(f\"[Frame {frame_idx}] processed.\")\n","\n","    cap.release()\n","    writer.release()\n","    print(\"Video saved to:\", out_path)\n"],"metadata":{"id":"pifH2Gg-C1X3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Make sure your model is on the right device and in eval mode\n","model.to(device)\n","model.eval()\n","\n","# Paths to your input & output videos\n","input_video_path  = '/content/drive/MyDrive/Projects/Car Detection v2/Video Test/traffic_test.mp4'\n","output_video_path = '/content/drive/MyDrive/Projects/Car Detection v2/Video Test Prediction Colab Env/singleclass_colab_Faster-RCNN_50epochs.mp4'\n","category_names = [\"__background__\", \"car\"]\n","\n","# Run the video through your Faster R-CNN\n","process_video(\n","    model,\n","    in_path    = input_video_path,\n","    out_path   = output_video_path,\n","    device     = device,\n","    score_thresh = 0.5,   # only draw boxes with score ≥ 0.5\n","    alpha        = 0.4,   # overlay transparency\n","    print_every  = 30,     # log every 30 frames\n","    category_names=category_names\n",")\n","\n","# After it finishes you should see:\n","# ✅ Video saved to: /content/.../TrafficPolice_detectedFRCNN.mp4\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j1nZn8weUdgB","executionInfo":{"status":"ok","timestamp":1749055546689,"user_tz":-420,"elapsed":820522,"user":{"displayName":"Samuel Ady Sanjaya","userId":"15801194796254517624"}},"outputId":"6eca7759-22dd-4c08-ee6a-2409192b58e7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[Frame 30] processed.\n","[Frame 60] processed.\n","[Frame 90] processed.\n","[Frame 120] processed.\n","[Frame 150] processed.\n","[Frame 180] processed.\n","[Frame 210] processed.\n","[Frame 240] processed.\n","[Frame 270] processed.\n","[Frame 300] processed.\n","[Frame 330] processed.\n","[Frame 360] processed.\n","[Frame 390] processed.\n","[Frame 420] processed.\n","[Frame 450] processed.\n","[Frame 480] processed.\n","[Frame 510] processed.\n","[Frame 540] processed.\n","[Frame 570] processed.\n","[Frame 600] processed.\n","[Frame 630] processed.\n","[Frame 660] processed.\n","[Frame 690] processed.\n","[Frame 720] processed.\n","[Frame 750] processed.\n","[Frame 780] processed.\n","[Frame 810] processed.\n","[Frame 840] processed.\n","[Frame 870] processed.\n","[Frame 900] processed.\n","[Frame 930] processed.\n","[Frame 960] processed.\n","[Frame 990] processed.\n","[Frame 1020] processed.\n","[Frame 1050] processed.\n","[Frame 1080] processed.\n","[Frame 1110] processed.\n","[Frame 1140] processed.\n","[Frame 1170] processed.\n","[Frame 1200] processed.\n","[Frame 1230] processed.\n","[Frame 1260] processed.\n","[Frame 1290] processed.\n","[Frame 1320] processed.\n","[Frame 1350] processed.\n","[Frame 1380] processed.\n","[Frame 1410] processed.\n","[Frame 1440] processed.\n","[Frame 1470] processed.\n","[Frame 1500] processed.\n","[Frame 1530] processed.\n","[Frame 1560] processed.\n","[Frame 1590] processed.\n","[Frame 1620] processed.\n","[Frame 1650] processed.\n","[Frame 1680] processed.\n","[Frame 1710] processed.\n","[Frame 1740] processed.\n","[Frame 1770] processed.\n","[Frame 1800] processed.\n","[Frame 1830] processed.\n","[Frame 1860] processed.\n","[Frame 1890] processed.\n","[Frame 1920] processed.\n","[Frame 1950] processed.\n","[Frame 1980] processed.\n","[Frame 2010] processed.\n","[Frame 2040] processed.\n","[Frame 2070] processed.\n","[Frame 2100] processed.\n","[Frame 2130] processed.\n","[Frame 2160] processed.\n","[Frame 2190] processed.\n","[Frame 2220] processed.\n","[Frame 2250] processed.\n","[Frame 2280] processed.\n","[Frame 2310] processed.\n","[Frame 2340] processed.\n","[Frame 2370] processed.\n","[Frame 2400] processed.\n","[Frame 2430] processed.\n","[Frame 2460] processed.\n","[Frame 2490] processed.\n","[Frame 2520] processed.\n","[Frame 2550] processed.\n","[Frame 2580] processed.\n","[Frame 2610] processed.\n","[Frame 2640] processed.\n","[Frame 2670] processed.\n","[Frame 2700] processed.\n","[Frame 2730] processed.\n","[Frame 2760] processed.\n","[Frame 2790] processed.\n","[Frame 2820] processed.\n","[Frame 2850] processed.\n","[Frame 2880] processed.\n","[Frame 2910] processed.\n","[Frame 2940] processed.\n","[Frame 2970] processed.\n","[Frame 3000] processed.\n","[Frame 3030] processed.\n","[Frame 3060] processed.\n","[Frame 3090] processed.\n","[Frame 3120] processed.\n","[Frame 3150] processed.\n","[Frame 3180] processed.\n","[Frame 3210] processed.\n","[Frame 3240] processed.\n","[Frame 3270] processed.\n","[Frame 3300] processed.\n","[Frame 3330] processed.\n","[Frame 3360] processed.\n","[Frame 3390] processed.\n","[Frame 3420] processed.\n","[Frame 3450] processed.\n","[Frame 3480] processed.\n","[Frame 3510] processed.\n","[Frame 3540] processed.\n","[Frame 3570] processed.\n","[Frame 3600] processed.\n","[Frame 3630] processed.\n","[Frame 3660] processed.\n","[Frame 3690] processed.\n","[Frame 3720] processed.\n","[Frame 3750] processed.\n","[Frame 3780] processed.\n","[Frame 3810] processed.\n","[Frame 3840] processed.\n","[Frame 3870] processed.\n","[Frame 3900] processed.\n","[Frame 3930] processed.\n","[Frame 3960] processed.\n","[Frame 3990] processed.\n","[Frame 4020] processed.\n","[Frame 4050] processed.\n","[Frame 4080] processed.\n","[Frame 4110] processed.\n","[Frame 4140] processed.\n","[Frame 4170] processed.\n","[Frame 4200] processed.\n","[Frame 4230] processed.\n","[Frame 4260] processed.\n","[Frame 4290] processed.\n","[Frame 4320] processed.\n","[Frame 4350] processed.\n","[Frame 4380] processed.\n","[Frame 4410] processed.\n","[Frame 4440] processed.\n","[Frame 4470] processed.\n","[Frame 4500] processed.\n","[Frame 4530] processed.\n","[Frame 4560] processed.\n","[Frame 4590] processed.\n","[Frame 4620] processed.\n","[Frame 4650] processed.\n","[Frame 4680] processed.\n","[Frame 4710] processed.\n","[Frame 4740] processed.\n","[Frame 4770] processed.\n","[Frame 4800] processed.\n","[Frame 4830] processed.\n","[Frame 4860] processed.\n","[Frame 4890] processed.\n","[Frame 4920] processed.\n","[Frame 4950] processed.\n","[Frame 4980] processed.\n","[Frame 5010] processed.\n","[Frame 5040] processed.\n","[Frame 5070] processed.\n","[Frame 5100] processed.\n","[Frame 5130] processed.\n","[Frame 5160] processed.\n","[Frame 5190] processed.\n","[Frame 5220] processed.\n","[Frame 5250] processed.\n","[Frame 5280] processed.\n","[Frame 5310] processed.\n","[Frame 5340] processed.\n","[Frame 5370] processed.\n","[Frame 5400] processed.\n","[Frame 5430] processed.\n","[Frame 5460] processed.\n","[Frame 5490] processed.\n","[Frame 5520] processed.\n","[Frame 5550] processed.\n","[Frame 5580] processed.\n","[Frame 5610] processed.\n","[Frame 5640] processed.\n","[Frame 5670] processed.\n","[Frame 5700] processed.\n","[Frame 5730] processed.\n","[Frame 5760] processed.\n","[Frame 5790] processed.\n","[Frame 5820] processed.\n","[Frame 5850] processed.\n","[Frame 5880] processed.\n","[Frame 5910] processed.\n","Video saved to: /content/drive/MyDrive/Projects/Car Detection v2/Video Test Prediction Colab Env/singleclass_colab_Faster-RCNN_50epochs.mp4\n"]}]},{"cell_type":"markdown","source":[],"metadata":{"id":"iXY7ORktcy7Q"}},{"cell_type":"markdown","source":[],"metadata":{"id":"AhLr1OX6cy4j"}},{"cell_type":"markdown","source":[],"metadata":{"id":"wWECfRzAcy1v"}},{"cell_type":"markdown","source":[],"metadata":{"id":"zqkbRPJncyyz"}},{"cell_type":"markdown","source":[],"metadata":{"id":"7XUgQdkHcywH"}},{"cell_type":"markdown","source":[],"metadata":{"id":"vpEtsAPPcytM"}},{"cell_type":"markdown","source":[],"metadata":{"id":"fbjAKyM0cyqQ"}},{"cell_type":"markdown","source":[],"metadata":{"id":"dFFyBxDPcynj"}},{"cell_type":"markdown","source":[],"metadata":{"id":"H-hYiX4acyk3"}},{"cell_type":"markdown","source":[],"metadata":{"id":"HlvDKe2Dcyh7"}},{"cell_type":"markdown","source":[],"metadata":{"id":"XlopEmdQcyfT"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"GhnNYJ8fcpQW"},"outputs":[],"source":[]}]}